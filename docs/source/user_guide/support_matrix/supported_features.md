# Supported Features

The feature support principle of vLLM Ascend is: **aligned with the vLLM**. We are also actively collaborating with the community to accelerate support.

Functional call: https://docs.vllm.ai/en/latest/features/tool_calling/

You can check the [support status of vLLM V1 Engine][v1_user_guide]. Below is the feature support status of vLLM Ascend:

| Feature                       |      Status    | Next Step                                                              |
|-------------------------------|----------------|------------------------------------------------------------------------|
| Chunked Prefill               | 游릭 Functional  | Functional, see detailed note: [Chunked Prefill][cp]                     |
| Automatic Prefix Caching      | 游릭 Functional  | Functional, see detailed note: [vllm-ascend#732][apc]                    |
| LoRA                          | 游릭 Functional  | [vllm-ascend#396][multilora], [vllm-ascend#893][v1 multilora]          |
| Speculative decoding          | 游릭 Functional  | Basic support                                                          |
| Pooling                       | 游릭 Functional  | CI needed to adapt to more models; V1 support relies on vLLM support.   |
| Enc-dec                       | 游리 Planned     | vLLM should support this feature first.                                |
| Multi Modality                | 游릭 Functional  | [Tutorial][multimodal], optimizing and adapting more models            |
| LogProbs                      | 游릭 Functional  | CI needed                                                              |
| Prompt logProbs               | 游릭 Functional  | CI needed                                                              |
| Async output                  | 游릭 Functional  | CI needed                                                              |
| Beam search                   | 游릭 Functional  | CI needed                                                              |
| Guided Decoding               | 游릭 Functional  | [vllm-ascend#177][guided_decoding]                                     |
| Tensor Parallel               | 游릭 Functional  | Make TP >4 work with graph mode.                                        |
| Pipeline Parallel             | 游릭 Functional  | Write official guide and tutorial.                                     |
| Expert Parallel               | 游릭 Functional  | Support dynamic EPLB.                                                  |
| Data Parallel                 | 游릭 Functional  | Data Parallel support for Qwen3 MoE.                                   |
| Prefill Decode Disaggregation | 游릭 Functional  | Functional, xPyD is supported.                                         |
| Quantization                  | 游릭 Functional  | W8A8 available; working on more quantization method support (W4A8, etc) |
| Graph Mode                    | 游댯 Experimental| Experimental, see detailed note: [vllm-ascend#767][graph_mode]           |
| Sleep Mode                    | 游릭 Functional  |                                                                        |
| Context Parallel              | 游댯 Experimental|                                                                        |

- 游릭 Functional: Fully operational, with ongoing optimizations.
- 游댯 Experimental: Experimental support, interfaces and functions may change.
- 游뚾 WIP: Under active development, will be supported soon.
- 游리 Planned: Scheduled for future implementation (some may have open PRs/RFCs).
- 游댮 NO plan/Deprecated: No plan or deprecated by vLLM.

[v1_user_guide]: https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html
[multimodal]: https://docs.vllm.ai/projects/ascend/en/latest/tutorials/single_npu_multimodal.html
[guided_decoding]: https://github.com/vllm-project/vllm-ascend/issues/177
[multilora]: https://github.com/vllm-project/vllm-ascend/issues/396
[v1 multilora]: https://github.com/vllm-project/vllm-ascend/pull/893
[graph_mode]: https://github.com/vllm-project/vllm-ascend/issues/767
[apc]: https://github.com/vllm-project/vllm-ascend/issues/732
[cp]: https://docs.vllm.ai/en/stable/performance/optimization.html#chunked-prefill
[1P1D]: https://github.com/vllm-project/vllm-ascend/pull/950
[ray]: https://github.com/vllm-project/vllm-ascend/issues/1751
