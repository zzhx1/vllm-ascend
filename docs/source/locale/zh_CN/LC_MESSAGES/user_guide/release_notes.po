# Translations template for PROJECT.
# Copyright (C) 2025 ORGANIZATION
# This file is distributed under the same license as the PROJECT project.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"
"POT-Creation-Date: 2025-07-18 09:01+0800\n"
"PO-Revision-Date: 2025-07-18 10:11+0800\n"
"Last-Translator: \n"
"Language-Team: \n"
"Language: zh\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"
"X-Generator: Poedit 3.5\n"

#: ../../user_guide/release_notes.md:1
msgid "Release note"
msgstr "ç‰ˆæœ¬è¯´æ˜"

#: ../../user_guide/release_notes.md:3
msgid "v0.9.2rc1 - 2025.07.11"
msgstr ""

#: ../../user_guide/release_notes.md:5
msgid ""
"This is the 1st release candidate of v0.9.2 for vLLM Ascend. Please follow "
"the [official doc](https://vllm-ascend.readthedocs.io/en/) to get started. "
"From this release, V1 engine will be enabled by default, there is no need "
"to set `VLLM_USE_V1=1` any more. And this release is the last version to "
"support V0 engine, V0 code will be clean up in the future."
msgstr ""
"è¿™æ˜¯ vLLM Ascend v0.9.2 çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https://vllm-"
"ascend.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚ä»æœ¬æ¬¡å‘å¸ƒèµ·ï¼ŒV1 å¼•æ“å°†é»˜è®¤å¯ç”¨ï¼Œä¸å†éœ€"
"è¦è®¾ç½® `VLLM_USE_V1=1`ã€‚æ­¤å¤–ï¼Œè¯¥ç‰ˆæœ¬ä¹Ÿæ˜¯æœ€åä¸€ä¸ªæ”¯æŒ V0 å¼•æ“çš„ç‰ˆæœ¬ï¼ŒV0 ç›¸å…³"
"ä»£ç å°†åœ¨æœªæ¥è¢«æ¸…ç†ã€‚"

#: ../../user_guide/release_notes.md:7 ../../user_guide/release_notes.md:34
#: ../../user_guide/release_notes.md:70 ../../user_guide/release_notes.md:78
#: ../../user_guide/release_notes.md:116 ../../user_guide/release_notes.md:140
#: ../../user_guide/release_notes.md:163 ../../user_guide/release_notes.md:186
#: ../../user_guide/release_notes.md:206 ../../user_guide/release_notes.md:231
#: ../../user_guide/release_notes.md:253 ../../user_guide/release_notes.md:285
msgid "Highlights"
msgstr "äº®ç‚¹"

#: ../../user_guide/release_notes.md:8
msgid ""
"Pooling model works with V1 engine now. You can take a try with Qwen3 "
"embedding model [#1359](https://github.com/vllm-project/vllm-ascend/"
"pull/1359)."
msgstr ""
"Pooling æ¨¡å‹ç°åœ¨å¯ä»¥ä¸ V1 å¼•æ“ä¸€èµ·ä½¿ç”¨ã€‚ä½ å¯ä»¥å°è¯•ä½¿ç”¨ Qwen3 embedding æ¨¡å‹ "
"[#1359](https://github.com/vllm-project/vllm-ascend/pull/1359)ã€‚"

#: ../../user_guide/release_notes.md:9
msgid ""
"The performance on Atlas 300I series has been improved. [#1591](https://"
"github.com/vllm-project/vllm-ascend/pull/1591)"
msgstr ""
"Atlas 300I ç³»åˆ—çš„æ€§èƒ½å·²ç»æå‡ã€‚ [#1591](https://github.com/vllm-project/"
"vllm-ascend/pull/1591)"

#: ../../user_guide/release_notes.md:10
msgid ""
"aclgraph mode works with Moe models now. Currently, only Qwen3 Moe is well "
"tested. [#1381](https://github.com/vllm-project/vllm-ascend/pull/1381)"
msgstr ""
"aclgraph æ¨¡å¼ç°åœ¨å¯ä»¥ä¸ Moe æ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚ç›®å‰ï¼Œä»…å¯¹ Qwen3 Moe è¿›è¡Œäº†å……åˆ†æµ‹"
"è¯•ã€‚[#1381](https://github.com/vllm-project/vllm-ascend/pull/1381)"

#: ../../user_guide/release_notes.md:12 ../../user_guide/release_notes.md:39
#: ../../user_guide/release_notes.md:83 ../../user_guide/release_notes.md:146
#: ../../user_guide/release_notes.md:168 ../../user_guide/release_notes.md:191
#: ../../user_guide/release_notes.md:212 ../../user_guide/release_notes.md:236
#: ../../user_guide/release_notes.md:258 ../../user_guide/release_notes.md:291
msgid "Core"
msgstr "æ ¸å¿ƒ"

#: ../../user_guide/release_notes.md:13
msgid ""
"Ascend PyTorch adapter (torch_npu) has been upgraded to `2.5.1.post1."
"dev20250619`. Donâ€™t forget to update it in your environment. [#1347]"
"(https://github.com/vllm-project/vllm-ascend/pull/1347)"
msgstr ""
"Ascend PyTorch é€‚é…å™¨ï¼ˆtorch_npuï¼‰å·²å‡çº§åˆ° `2.5.1.post1.dev20250619`ã€‚è¯·ä¸è¦"
"å¿˜è®°åœ¨æ‚¨çš„ç¯å¢ƒä¸­è¿›è¡Œæ›´æ–°ã€‚ [#1347](https://github.com/vllm-project/vllm-"
"ascend/pull/1347)"

#: ../../user_guide/release_notes.md:14
msgid ""
"The **GatherV3** error has been fixed with **aclgraph** mode. [#1416]"
"(https://github.com/vllm-project/vllm-ascend/pull/1416)"
msgstr ""
"**GatherV3** é”™è¯¯å·²é€šè¿‡ **aclgraph** æ¨¡å¼ä¿®å¤ã€‚[#1416](https://github.com/"
"vllm-project/vllm-ascend/pull/1416)"

#: ../../user_guide/release_notes.md:15
msgid ""
"W8A8 quantization works on Atlas 300I series now. [#1560](https://github."
"com/vllm-project/vllm-ascend/pull/1560)"
msgstr ""
"W8A8 é‡åŒ–ç°åœ¨å¯ä»¥åœ¨ Atlas 300I ç³»åˆ—ä¸Šè¿è¡Œäº†ã€‚[#1560](https://github.com/"
"vllm-project/vllm-ascend/pull/1560)"

#: ../../user_guide/release_notes.md:16
msgid ""
"Fix the accuracy problem with deploy models with parallel parameters. "
"[#1678](https://github.com/vllm-project/vllm-ascend/pull/1678)"
msgstr ""
"ä¿®å¤äº†ä½¿ç”¨å¹¶è¡Œå‚æ•°éƒ¨ç½²æ¨¡å‹æ—¶çš„å‡†ç¡®æ€§é—®é¢˜ã€‚[#1678](https://github.com/vllm-"
"project/vllm-ascend/pull/1678)"

#: ../../user_guide/release_notes.md:17
msgid ""
"The pre-built wheel package now requires lower version of glibc. Users can "
"use it by `pip install vllm-ascend` directly. [#1582](https://github.com/"
"vllm-project/vllm-ascend/pull/1582)"
msgstr ""
"é¢„ç¼–è¯‘çš„ wheel åŒ…ç°åœ¨è¦æ±‚æ›´ä½ç‰ˆæœ¬çš„ glibcã€‚ç”¨æˆ·å¯ä»¥ç›´æ¥é€šè¿‡ `pip install "
"vllm-ascend` ä½¿ç”¨å®ƒã€‚[#1582](https://github.com/vllm-project/vllm-ascend/"
"pull/1582)"

#: ../../user_guide/release_notes.md:19 ../../user_guide/release_notes.md:99
#: ../../user_guide/release_notes.md:153 ../../user_guide/release_notes.md:177
#: ../../user_guide/release_notes.md:195 ../../user_guide/release_notes.md:219
#: ../../user_guide/release_notes.md:242 ../../user_guide/release_notes.md:266
#: ../../user_guide/release_notes.md:296
msgid "Other"
msgstr "å…¶å®ƒ"

#: ../../user_guide/release_notes.md:20
msgid ""
"Official doc has been updated for better read experience. For example, more "
"deployment tutorials are added, user/developer docs are updated. More guide "
"will coming soon."
msgstr ""
"å®˜æ–¹æ–‡æ¡£å·²æ›´æ–°ï¼Œä»¥æå‡é˜…è¯»ä½“éªŒã€‚ä¾‹å¦‚ï¼Œå¢åŠ äº†æ›´å¤šéƒ¨ç½²æ•™ç¨‹ï¼Œç”¨æˆ·/å¼€å‘è€…æ–‡æ¡£å·²"
"æ›´æ–°ã€‚æ›´å¤šæŒ‡å—å³å°†æ¨å‡ºã€‚"

#: ../../user_guide/release_notes.md:21
msgid ""
"Fix accuracy problem for deepseek V3/R1 models with torchair graph in long "
"sequence predictions. [#1331](https://github.com/vllm-project/vllm-ascend/"
"pull/1331)"
msgstr ""
"ä¿®å¤ deepseek V3/R1 æ¨¡å‹åœ¨ä½¿ç”¨ torchair å›¾è¿›è¡Œé•¿åºåˆ—é¢„æµ‹æ—¶çš„ç²¾åº¦é—®é¢˜ã€‚"
"[#1331](https://github.com/vllm-project/vllm-ascend/pull/1331)"

#: ../../user_guide/release_notes.md:22
msgid ""
"A new env variable `VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP` has been added. "
"It enables the fused allgather-experts kernel for Deepseek V3/R1 models. "
"The default value is `0`. [#1335](https://github.com/vllm-project/vllm-"
"ascend/pull/1335)"
msgstr ""
"æ–°å¢äº†ä¸€ä¸ªç¯å¢ƒå˜é‡ `VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP`ã€‚å®ƒç”¨äºå¯ç”¨ "
"Deepseek V3/R1 æ¨¡å‹çš„ fused allgather-experts å†…æ ¸ã€‚é»˜è®¤å€¼ä¸º `0`ã€‚[#1335]"
"(https://github.com/vllm-project/vllm-ascend/pull/1335)"

#: ../../user_guide/release_notes.md:23
msgid ""
"A new env variable `VLLM_ASCEND_ENABLE_TOPK_TOPP_OPTIMIZATION` has been "
"added to improve the performance of topk-topp sampling. The default value "
"is 0, we'll consider to enable it by default in the future[#1732](https://"
"github.com/vllm-project/vllm-ascend/pull/1732)"
msgstr ""
"æ–°å¢äº†ä¸€ä¸ªç¯å¢ƒå˜é‡ `VLLM_ASCEND_ENABLE_TOPK_TOPP_OPTIMIZATION`ï¼Œç”¨äºæå‡ "
"topk-topp é‡‡æ ·çš„æ€§èƒ½ã€‚è¯¥å˜é‡é»˜è®¤å€¼ä¸º 0ï¼Œæœªæ¥æˆ‘ä»¬ä¼šè€ƒè™‘é»˜è®¤å¯ç”¨æ­¤é€‰é¡¹[#1732]"
"(https://github.com/vllm-project/vllm-ascend/pull/1732)ã€‚"

#: ../../user_guide/release_notes.md:24
msgid ""
"A batch of bugs have been fixed for Data Parallelism case [#1273](https://"
"github.com/vllm-project/vllm-ascend/pull/1273) [#1322](https://github.com/"
"vllm-project/vllm-ascend/pull/1322) [#1275](https://github.com/vllm-project/"
"vllm-ascend/pull/1275) [#1478](https://github.com/vllm-project/vllm-ascend/"
"pull/1478)"
msgstr ""
"å·²ä¿®å¤äº†ä¸€æ‰¹ä¸æ•°æ®å¹¶è¡Œç›¸å…³çš„ bug [#1273](https://github.com/vllm-project/"
"vllm-ascend/pull/1273) [#1322](https://github.com/vllm-project/vllm-ascend/"
"pull/1322) [#1275](https://github.com/vllm-project/vllm-ascend/pull/1275) "
"[#1478](https://github.com/vllm-project/vllm-ascend/pull/1478)"

#: ../../user_guide/release_notes.md:25
msgid ""
"The DeepSeek performance has been improved. [#1194](https://github.com/vllm-"
"project/vllm-ascend/pull/1194) [#1395](https://github.com/vllm-project/vllm-"
"ascend/pull/1395) [#1380](https://github.com/vllm-project/vllm-ascend/"
"pull/1380)"
msgstr ""
"DeepSeek çš„æ€§èƒ½å·²å¾—åˆ°æå‡ã€‚[#1194](https://github.com/vllm-project/vllm-"
"ascend/pull/1194) [#1395](https://github.com/vllm-project/vllm-ascend/"
"pull/1395) [#1380](https://github.com/vllm-project/vllm-ascend/pull/1380)"

#: ../../user_guide/release_notes.md:26
msgid ""
"Ascend scheduler works with prefix cache now. [#1446](https://github.com/"
"vllm-project/vllm-ascend/pull/1446)"
msgstr ""
"Ascend è°ƒåº¦å™¨ç°åœ¨æ”¯æŒå‰ç¼€ç¼“å­˜ã€‚[#1446](https://github.com/vllm-project/vllm-"
"ascend/pull/1446)"

#: ../../user_guide/release_notes.md:27
msgid ""
"DeepSeek now works with prefix cache now. [#1498](https://github.com/vllm-"
"project/vllm-ascend/pull/1498)"
msgstr ""
"DeepSeek ç°åœ¨æ”¯æŒå‰ç¼€ç¼“å­˜äº†ã€‚[#1498](https://github.com/vllm-project/vllm-"
"ascend/pull/1498)"

#: ../../user_guide/release_notes.md:28
msgid ""
"Support prompt logprobs to recover ceval accuracy in V1 [#1483](https://"
"github.com/vllm-project/vllm-ascend/pull/1483)"
msgstr ""
"æ”¯æŒä½¿ç”¨ prompt logprobs æ¢å¤ V1 çš„ ceval å‡†ç¡®ç‡ [#1483](https://github.com/"
"vllm-project/vllm-ascend/pull/1483)"

#: ../../user_guide/release_notes.md:30
msgid "v0.9.1rc1 - 2025.06.22"
msgstr "v0.9.1rc1 - 2025.06.22"

#: ../../user_guide/release_notes.md:32
msgid ""
"This is the 1st release candidate of v0.9.1 for vLLM Ascend. Please follow "
"the [official doc](https://vllm-ascend.readthedocs.io/en/) to get started."
msgstr ""
"è¿™æ˜¯ vLLM Ascend v0.9.1 çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
"ascend.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚"

#: ../../user_guide/release_notes.md:36
msgid ""
"Atlas 300I series is experimental supported in this release. [#1333]"
"(https://github.com/vllm-project/vllm-ascend/pull/1333) After careful "
"consideration, this feature **will NOT be included in v0.9.1-dev branch** "
"taking into account the v0.9.1 release quality and the feature rapid "
"iteration to improve performance on Atlas 300I series. We will improve this "
"from 0.9.2rc1 and later."
msgstr ""
"æœ¬ç‰ˆæœ¬å¯¹ Atlas 300I ç³»åˆ—æä¾›äº†å®éªŒæ€§æ”¯æŒã€‚[#1333](https://github.com/vllm-"
"project/vllm-ascend/pull/1333) ç»è¿‡æ…é‡è€ƒè™‘ï¼Œé‰´äº v0.9.1 ç‰ˆæœ¬å‘å¸ƒçš„è´¨é‡è¦æ±‚"
"ä»¥åŠ Atlas 300I ç³»åˆ—æ€§èƒ½ä¼˜åŒ–çš„å¿«é€Ÿè¿­ä»£ï¼Œè¯¥åŠŸèƒ½**ä¸ä¼šè¢«åŒ…å«åœ¨ v0.9.1-dev åˆ†æ”¯"
"ä¸­**ã€‚æˆ‘ä»¬å°†åœ¨ 0.9.2rc1 åŠä¹‹åçš„ç‰ˆæœ¬ä¸­è¿›ä¸€æ­¥å®Œå–„è¯¥åŠŸèƒ½ã€‚"

#: ../../user_guide/release_notes.md:37
msgid ""
"Support EAGLE-3 for speculative decoding. [#1032](https://github.com/vllm-"
"project/vllm-ascend/pull/1032)"
msgstr ""
"æ”¯æŒ EAGLE-3 è¿›è¡Œæ¨æµ‹å¼è§£ç ã€‚[#1032](https://github.com/vllm-project/vllm-"
"ascend/pull/1032)"

#: ../../user_guide/release_notes.md:40
msgid ""
"Ascend PyTorch adapter (torch_npu) has been upgraded to `2.5.1.post1."
"dev20250528`. Donâ€™t forget to update it in your environment. [#1235]"
"(https://github.com/vllm-project/vllm-ascend/pull/1235)"
msgstr ""
"Ascend PyTorch é€‚é…å™¨ï¼ˆtorch_npuï¼‰å·²å‡çº§åˆ° `2.5.1.post1.dev20250528`ã€‚è¯·ä¸è¦"
"å¿˜è®°åœ¨æ‚¨çš„ç¯å¢ƒä¸­è¿›è¡Œæ›´æ–°ã€‚[#1235](https://github.com/vllm-project/vllm-"
"ascend/pull/1235)"

#: ../../user_guide/release_notes.md:41
msgid ""
"Support Atlas 300I series container image. You can get it from [quay.io]"
"(https://quay.io/repository/vllm/vllm-ascend)"
msgstr ""
"æ”¯æŒAtlas 300Iç³»åˆ—çš„å®¹å™¨é•œåƒã€‚ä½ å¯ä»¥ä»[quay.io](https://quay.io/repository/"
"vllm/vllm-ascend)è·å–ã€‚"

#: ../../user_guide/release_notes.md:42
msgid ""
"Fix token-wise padding mechanism to make multi-card graph mode work. [#1300]"
"(https://github.com/vllm-project/vllm-ascend/pull/1300)"
msgstr ""
"ä¿®å¤æŒ‰ token å¡«å……æœºåˆ¶ä»¥æ”¯æŒå¤šå¡å›¾æ¨¡å¼ã€‚ [#1300](https://github.com/vllm-"
"project/vllm-ascend/pull/1300)"

#: ../../user_guide/release_notes.md:43
msgid ""
"Upgrade vllm to 0.9.1 [#1165]https://github.com/vllm-project/vllm-ascend/"
"pull/1165"
msgstr ""
"å°† vllm å‡çº§åˆ° 0.9.1 [#1165]https://github.com/vllm-project/vllm-ascend/"
"pull/1165"

#: ../../user_guide/release_notes.md:45
msgid "Other Improvements"
msgstr "å…¶ä»–æ”¹è¿›"

#: ../../user_guide/release_notes.md:46
msgid ""
"Initial support Chunked Prefill for MLA. [#1172](https://github.com/vllm-"
"project/vllm-ascend/pull/1172)"
msgstr ""
"ä¸ºMLAåˆæ­¥æ”¯æŒåˆ†å—é¢„å¡«å……ã€‚ [#1172](https://github.com/vllm-project/vllm-"
"ascend/pull/1172)"

#: ../../user_guide/release_notes.md:47
msgid ""
"An example of best practices to run DeepSeek with ETP has been added. "
"[#1101](https://github.com/vllm-project/vllm-ascend/pull/1101)"
msgstr ""
"å·²æ–°å¢ä¸€ä¸ªä½¿ç”¨ ETP è¿è¡Œ DeepSeek çš„æœ€ä½³å®è·µç¤ºä¾‹ã€‚[#1101](https://github.com/"
"vllm-project/vllm-ascend/pull/1101)"

#: ../../user_guide/release_notes.md:48
msgid ""
"Performance improvements for DeepSeek using the TorchAir graph. [#1098]"
"(https://github.com/vllm-project/vllm-ascend/pull/1098), [#1131](https://"
"github.com/vllm-project/vllm-ascend/pull/1131)"
msgstr ""
"é€šè¿‡ä½¿ç”¨ TorchAir å›¾å¯¹ DeepSeek è¿›è¡Œäº†æ€§èƒ½æå‡ã€‚[#1098](https://github.com/"
"vllm-project/vllm-ascend/pull/1098), [#1131](https://github.com/vllm-"
"project/vllm-ascend/pull/1131)"

#: ../../user_guide/release_notes.md:49
msgid ""
"Supports the speculative decoding feature with AscendScheduler. [#943]"
"(https://github.com/vllm-project/vllm-ascend/pull/943)"
msgstr ""
"æ”¯æŒ AscendScheduler çš„é¢„æµ‹æ€§è§£ç åŠŸèƒ½ã€‚[#943](https://github.com/vllm-"
"project/vllm-ascend/pull/943)"

#: ../../user_guide/release_notes.md:50
msgid ""
"Improve `VocabParallelEmbedding` custom op performance. It will be enabled "
"in the next release. [#796](https://github.com/vllm-project/vllm-ascend/"
"pull/796)"
msgstr ""
"æå‡ `VocabParallelEmbedding` è‡ªå®šä¹‰ç®—å­çš„æ€§èƒ½ã€‚è¯¥ä¼˜åŒ–å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­å¯ç”¨ã€‚"
"[#796](https://github.com/vllm-project/vllm-ascend/pull/796)"

#: ../../user_guide/release_notes.md:51
msgid ""
"Fixed a device discovery and setup bug when running vLLM Ascend on Ray "
"[#884](https://github.com/vllm-project/vllm-ascend/pull/884)"
msgstr ""
"ä¿®å¤äº†åœ¨ Ray ä¸Šè¿è¡Œ vLLM Ascend æ—¶çš„è®¾å¤‡å‘ç°å’Œè®¾ç½®é”™è¯¯ [#884](https://"
"github.com/vllm-project/vllm-ascend/pull/884)"

#: ../../user_guide/release_notes.md:52
msgid ""
"DeepSeek with [MC2](https://www.hiascend.com/document/detail/zh/"
"canncommercial/81RC1/developmentguide/opdevg/ascendcbestP/"
"atlas_ascendc_best_practices_10_0043.html) (Merged Compute and "
"Communication) now works properly. [#1268](https://github.com/vllm-project/"
"vllm-ascend/pull/1268)"
msgstr ""
"DeepSeek ç°å·²å¯ä»¥ä¸ [MC2](https://www.hiascend.com/document/detail/zh/"
"canncommercial/81RC1/developmentguide/opdevg/ascendcbestP/"
"atlas_ascendc_best_practices_10_0043.html)ï¼ˆè®¡ç®—ä¸é€šä¿¡èåˆï¼‰æ­£å¸¸å·¥ä½œã€‚"
"[#1268](https://github.com/vllm-project/vllm-ascend/pull/1268)"

#: ../../user_guide/release_notes.md:53
msgid ""
"Fixed log2phy NoneType bug with static EPLB feature. [#1186](https://github."
"com/vllm-project/vllm-ascend/pull/1186)"
msgstr ""
"ä¿®å¤äº†å¸¦æœ‰é™æ€ EPLB ç‰¹æ€§æ—¶ log2phy ä¸º NoneType çš„ bugã€‚[#1186](https://"
"github.com/vllm-project/vllm-ascend/pull/1186)"

#: ../../user_guide/release_notes.md:54
msgid ""
"Improved performance for DeepSeek with DBO enabled. [#997](https://github."
"com/vllm-project/vllm-ascend/pull/997), [#1135](https://github.com/vllm-"
"project/vllm-ascend/pull/1135)"
msgstr ""
"å¯ç”¨ DBO åï¼ŒDeepSeek çš„æ€§èƒ½å¾—åˆ°æå‡ã€‚[#997](https://github.com/vllm-"
"project/vllm-ascend/pull/997)ï¼Œ[#1135](https://github.com/vllm-project/vllm-"
"ascend/pull/1135)"

#: ../../user_guide/release_notes.md:55
msgid ""
"Refactoring AscendFusedMoE [#1229](https://github.com/vllm-project/vllm-"
"ascend/pull/1229)"
msgstr ""
"é‡æ„ AscendFusedMoE [#1229](https://github.com/vllm-project/vllm-ascend/"
"pull/1229)"

#: ../../user_guide/release_notes.md:56
msgid ""
"Add initial user stories page (include LLaMA-Factory/TRL/verl/MindIE Turbo/"
"GPUStack) [#1224](https://github.com/vllm-project/vllm-ascend/pull/1224)"
msgstr ""
"æ–°å¢åˆå§‹ç”¨æˆ·æ•…äº‹é¡µé¢ï¼ˆåŒ…æ‹¬ LLaMA-Factory/TRL/verl/MindIE Turbo/GPUStackï¼‰"
"[#1224](https://github.com/vllm-project/vllm-ascend/pull/1224)"

#: ../../user_guide/release_notes.md:57
msgid ""
"Add unit test framework [#1201](https://github.com/vllm-project/vllm-ascend/"
"pull/1201)"
msgstr ""
"æ·»åŠ å•å…ƒæµ‹è¯•æ¡†æ¶ [#1201](https://github.com/vllm-project/vllm-ascend/"
"pull/1201)"

#: ../../user_guide/release_notes.md:59
msgid "Known Issues"
msgstr "å·²çŸ¥é—®é¢˜"

#: ../../user_guide/release_notes.md:60
msgid ""
"In some cases, the vLLM process may crash with a **GatherV3** error when "
"**aclgraph** is enabled. We are working on this issue and will fix it in "
"the next release. [#1038](https://github.com/vllm-project/vllm-ascend/"
"issues/1038)"
msgstr ""
"åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå½“å¯ç”¨ **aclgraph** æ—¶ï¼ŒvLLM è¿›ç¨‹å¯èƒ½ä¼šå›  **GatherV3** é”™è¯¯è€Œ"
"å´©æºƒã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³æ­¤é—®é¢˜ï¼Œå¹¶å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä¿®å¤ã€‚[#1038](https://github."
"com/vllm-project/vllm-ascend/issues/1038)"

#: ../../user_guide/release_notes.md:61
msgid ""
"Prefix cache feature does not work with the Ascend Scheduler but without "
"chunked prefill enabled. This will be fixed in the next release. [#1350]"
"(https://github.com/vllm-project/vllm-ascend/issues/1350)"
msgstr ""
"å‰ç¼€ç¼“å­˜åŠŸèƒ½åœ¨æœªå¯ç”¨åˆ†å—é¢„å¡«å……çš„æƒ…å†µä¸‹æ— æ³•ä¸ Ascend è°ƒåº¦å™¨ä¸€åŒå·¥ä½œã€‚æ­¤é—®é¢˜å°†"
"åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä¿®å¤ã€‚[#1350](https://github.com/vllm-project/vllm-ascend/"
"issues/1350)"

#: ../../user_guide/release_notes.md:63
msgid "Full Changelog"
msgstr "å®Œæ•´æ›´æ–°æ—¥å¿—"

#: ../../user_guide/release_notes.md:64
msgid ""
"https://github.com/vllm-project/vllm-ascend/compare/v0.9.0rc2...v0.9.1rc1"
msgstr ""
"https://github.com/vllm-project/vllm-ascend/compare/v0.9.0rc2...v0.9.1rc1"

#: ../../user_guide/release_notes.md:66
msgid "v0.9.0rc2 - 2025.06.10"
msgstr "v0.9.0rc2 - 2025.06.10"

#: ../../user_guide/release_notes.md:68
msgid ""
"This release contains some quick fixes for v0.9.0rc1. Please use this "
"release instead of v0.9.0rc1."
msgstr ""
"æœ¬æ¬¡å‘å¸ƒåŒ…å«äº†ä¸€äº›é’ˆå¯¹ v0.9.0rc1 çš„å¿«é€Ÿä¿®å¤ã€‚è¯·ä½¿ç”¨æœ¬æ¬¡å‘å¸ƒç‰ˆæœ¬ï¼Œè€Œä¸æ˜¯ "
"v0.9.0rc1ã€‚"

#: ../../user_guide/release_notes.md:72
msgid ""
"Fix the import error when vllm-ascend is installed without editable way. "
"[#1152](https://github.com/vllm-project/vllm-ascend/pull/1152)"
msgstr ""
"ä¿®å¤å½“ä»¥éå¯ç¼–è¾‘æ–¹å¼å®‰è£… vllm-ascend æ—¶çš„å¯¼å…¥é”™è¯¯ã€‚[#1152](https://github."
"com/vllm-project/vllm-ascend/pull/1152)"

#: ../../user_guide/release_notes.md:74
msgid "v0.9.0rc1 - 2025.06.09"
msgstr "v0.9.0rc1 - 2025.06.09"

#: ../../user_guide/release_notes.md:76
msgid ""
"This is the 1st release candidate of v0.9.0 for vllm-ascend. Please follow "
"the [official doc](https://vllm-ascend.readthedocs.io/en/) to start the "
"journey. From this release, V1 Engine is recommended to use. The code of V0 "
"Engine is frozen and will not be maintained any more. Please set "
"environment `VLLM_USE_V1=1` to enable V1 Engine."
msgstr ""
"è¿™æ˜¯ vllm-ascend v0.9.0 çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
"ascend.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚ä»æ­¤ç‰ˆæœ¬èµ·ï¼Œæ¨èä½¿ç”¨ V1 å¼•æ“ã€‚V0 å¼•æ“çš„"
"ä»£ç å·²è¢«å†»ç»“ï¼Œä¸å†ç»´æŠ¤ã€‚å¦‚éœ€å¯ç”¨ V1 å¼•æ“ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ `VLLM_USE_V1=1`ã€‚"

#: ../../user_guide/release_notes.md:80
msgid ""
"DeepSeek works with graph mode now. Follow the [official doc](https://vllm-"
"ascend.readthedocs.io/en/latest/user_guide/feature_guide/graph_mode.html) "
"to take a try. [#789](https://github.com/vllm-project/vllm-ascend/pull/789)"
msgstr ""
"DeepSeek ç°åœ¨å·²æ”¯æŒå›¾æ¨¡å¼ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-ascend.readthedocs."
"io/en/latest/user_guide/feature_guide/graph_mode.html)è¿›è¡Œå°è¯•ã€‚[#789]"
"(https://github.com/vllm-project/vllm-ascend/pull/789)"

#: ../../user_guide/release_notes.md:81
msgid ""
"Qwen series models works with graph mode now. It works by default with V1 "
"Engine. Please note that in this release, only Qwen series models are well "
"tested with graph mode. We'll make it stable and generalize in the next "
"release. If you hit any issues, please feel free to open an issue on GitHub "
"and fallback to eager mode temporarily by set `enforce_eager=True` when "
"initializing the model."
msgstr ""
"Qwen ç³»åˆ—æ¨¡å‹ç°åœ¨æ”¯æŒå›¾æ¨¡å¼ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒåœ¨ V1 å¼•æ“ä¸‹è¿è¡Œã€‚è¯·æ³¨æ„ï¼Œæœ¬æ¬¡å‘"
"å¸ƒä¸­ï¼Œä»… Qwen ç³»åˆ—æ¨¡å‹ç»è¿‡äº†å……åˆ†çš„å›¾æ¨¡å¼æµ‹è¯•ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­è¿›ä¸€æ­¥æå‡"
"å…¶ç¨³å®šæ€§å¹¶æ¨å¹¿è‡³æ›´å¹¿æ³›çš„åœºæ™¯ã€‚å¦‚æœä½ é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶åœ¨ GitHub ä¸Šæäº¤ "
"issueï¼Œå¹¶åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶é€šè¿‡è®¾ç½® `enforce_eager=True` ä¸´æ—¶åˆ‡æ¢å› eager æ¨¡å¼ã€‚"

#: ../../user_guide/release_notes.md:85
msgid ""
"The performance of multi-step scheduler has been improved. Thanks for the "
"contribution from China Merchants Bank. [#814](https://github.com/vllm-"
"project/vllm-ascend/pull/814)"
msgstr ""
"å¤šæ­¥è°ƒåº¦å™¨çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚æ„Ÿè°¢æ‹›å•†é“¶è¡Œçš„è´¡çŒ®ã€‚[#814](https://github.com/"
"vllm-project/vllm-ascend/pull/814)"

#: ../../user_guide/release_notes.md:86
msgid ""
"LoRAã€Multi-LoRA And Dynamic Serving is supported for V1 Engine now. Thanks "
"for the contribution from China Merchants Bank. [#893](https://github.com/"
"vllm-project/vllm-ascend/pull/893)"
msgstr ""
"V1 å¼•æ“ç°åœ¨æ”¯æŒ LoRAã€å¤š LoRA ä»¥åŠåŠ¨æ€æœåŠ¡ã€‚æ„Ÿè°¢æ‹›å•†é“¶è¡Œçš„è´¡çŒ®ã€‚[#893]"
"(https://github.com/vllm-project/vllm-ascend/pull/893)"

#: ../../user_guide/release_notes.md:87
msgid ""
"Prefix cache and chunked prefill feature works now [#782](https://github."
"com/vllm-project/vllm-ascend/pull/782) [#844](https://github.com/vllm-"
"project/vllm-ascend/pull/844)"
msgstr ""
"å‰ç¼€ç¼“å­˜å’Œåˆ†å—é¢„å¡«å……åŠŸèƒ½ç°å·²å¯ç”¨ [#782](https://github.com/vllm-project/"
"vllm-ascend/pull/782) [#844](https://github.com/vllm-project/vllm-ascend/"
"pull/844)"

#: ../../user_guide/release_notes.md:88
msgid ""
"Spec decode and MTP features work with V1 Engine now. [#874](https://github."
"com/vllm-project/vllm-ascend/pull/874) [#890](https://github.com/vllm-"
"project/vllm-ascend/pull/890)"
msgstr ""
"Spec è§£ç å’Œ MTP åŠŸèƒ½ç°åœ¨å·²ç»æ”¯æŒ V1 å¼•æ“ã€‚[#874](https://github.com/vllm-"
"project/vllm-ascend/pull/874) [#890](https://github.com/vllm-project/vllm-"
"ascend/pull/890)"

#: ../../user_guide/release_notes.md:89
msgid ""
"DP feature works with DeepSeek now. [#1012](https://github.com/vllm-project/"
"vllm-ascend/pull/1012)"
msgstr ""
"DP åŠŸèƒ½ç°åœ¨å¯ä»¥ä¸ DeepSeek ä¸€èµ·ä½¿ç”¨ã€‚[#1012](https://github.com/vllm-"
"project/vllm-ascend/pull/1012)"

#: ../../user_guide/release_notes.md:90
msgid ""
"Input embedding feature works with V0 Engine now. [#916](https://github.com/"
"vllm-project/vllm-ascend/pull/916)"
msgstr ""
"è¾“å…¥åµŒå…¥ç‰¹æ€§ç°åœ¨å·²æ”¯æŒ V0 å¼•æ“ã€‚[#916](https://github.com/vllm-project/vllm-"
"ascend/pull/916)"

#: ../../user_guide/release_notes.md:91
msgid ""
"Sleep mode feature works with V1 Engine now. [#1084](https://github.com/"
"vllm-project/vllm-ascend/pull/1084)"
msgstr ""
"ä¼‘çœ æ¨¡å¼åŠŸèƒ½ç°åœ¨å·²æ”¯æŒ V1 å¼•æ“ã€‚[#1084](https://github.com/vllm-project/"
"vllm-ascend/pull/1084)"

#: ../../user_guide/release_notes.md:93 ../../user_guide/release_notes.md:149
#: ../../user_guide/release_notes.md:239 ../../user_guide/release_notes.md:262
msgid "Model"
msgstr "æ¨¡å‹"

#: ../../user_guide/release_notes.md:95
msgid ""
"Qwen2.5 VL works with V1 Engine now. [#736](https://github.com/vllm-project/"
"vllm-ascend/pull/736)"
msgstr ""
"Qwen2.5 VL ç°åœ¨å¯ä»¥ä¸ V1 å¼•æ“ååŒå·¥ä½œã€‚[#736](https://github.com/vllm-"
"project/vllm-ascend/pull/736)"

#: ../../user_guide/release_notes.md:96
msgid ""
"LLama4 works now. [#740](https://github.com/vllm-project/vllm-ascend/"
"pull/740)"
msgstr ""
"LLama4 ç°åœ¨å¯ä»¥ä½¿ç”¨äº†ã€‚[#740](https://github.com/vllm-project/vllm-ascend/"
"pull/740)"

#: ../../user_guide/release_notes.md:97
msgid ""
"A new kind of DeepSeek model called dual-batch overlap(DBO) is added. "
"Please set `VLLM_ASCEND_ENABLE_DBO=1` to use it. [#941](https://github.com/"
"vllm-project/vllm-ascend/pull/941)"
msgstr ""
"æ–°å¢äº†ä¸€ç§åä¸ºåŒæ‰¹æ¬¡é‡å ï¼ˆdual-batch overlapï¼ŒDBOï¼‰çš„ DeepSeek æ¨¡å‹ã€‚è¯·è®¾ç½® "
"`VLLM_ASCEND_ENABLE_DBO=1` ä»¥å¯ç”¨ã€‚ [#941](https://github.com/vllm-project/"
"vllm-ascend/pull/941)"

#: ../../user_guide/release_notes.md:101
msgid ""
"online serve with ascend quantization works now. [#877](https://github.com/"
"vllm-project/vllm-ascend/pull/877)"
msgstr ""
"åœ¨çº¿æœåŠ¡ç°å·²æ”¯æŒAscendé‡åŒ–ã€‚[#877](https://github.com/vllm-project/vllm-"
"ascend/pull/877)"

#: ../../user_guide/release_notes.md:102
msgid ""
"A batch of bugs for graph mode and moe model have been fixed. [#773]"
"(https://github.com/vllm-project/vllm-ascend/pull/773) [#771](https://"
"github.com/vllm-project/vllm-ascend/pull/771) [#774](https://github.com/"
"vllm-project/vllm-ascend/pull/774) [#816](https://github.com/vllm-project/"
"vllm-ascend/pull/816) [#817](https://github.com/vllm-project/vllm-ascend/"
"pull/817) [#819](https://github.com/vllm-project/vllm-ascend/pull/819) "
"[#912](https://github.com/vllm-project/vllm-ascend/pull/912) [#897](https://"
"github.com/vllm-project/vllm-ascend/pull/897) [#961](https://github.com/"
"vllm-project/vllm-ascend/pull/961) [#958](https://github.com/vllm-project/"
"vllm-ascend/pull/958) [#913](https://github.com/vllm-project/vllm-ascend/"
"pull/913) [#905](https://github.com/vllm-project/vllm-ascend/pull/905)"
msgstr ""
"å·²ä¿®å¤ä¸€æ‰¹å…³äºå›¾æ¨¡å¼å’Œmoeæ¨¡å‹çš„bugã€‚[#773](https://github.com/vllm-project/"
"vllm-ascend/pull/773) [#771](https://github.com/vllm-project/vllm-ascend/"
"pull/771) [#774](https://github.com/vllm-project/vllm-ascend/pull/774) "
"[#816](https://github.com/vllm-project/vllm-ascend/pull/816) [#817](https://"
"github.com/vllm-project/vllm-ascend/pull/817) [#819](https://github.com/"
"vllm-project/vllm-ascend/pull/819) [#912](https://github.com/vllm-project/"
"vllm-ascend/pull/912) [#897](https://github.com/vllm-project/vllm-ascend/"
"pull/897) [#961](https://github.com/vllm-project/vllm-ascend/pull/961) "
"[#958](https://github.com/vllm-project/vllm-ascend/pull/958) [#913](https://"
"github.com/vllm-project/vllm-ascend/pull/913) [#905](https://github.com/"
"vllm-project/vllm-ascend/pull/905)"

#: ../../user_guide/release_notes.md:103
msgid ""
"A batch of performance improvement PRs have been merged. [#784](https://"
"github.com/vllm-project/vllm-ascend/pull/784) [#803](https://github.com/"
"vllm-project/vllm-ascend/pull/803) [#966](https://github.com/vllm-project/"
"vllm-ascend/pull/966) [#839](https://github.com/vllm-project/vllm-ascend/"
"pull/839) [#970](https://github.com/vllm-project/vllm-ascend/pull/970) "
"[#947](https://github.com/vllm-project/vllm-ascend/pull/947) [#987](https://"
"github.com/vllm-project/vllm-ascend/pull/987) [#1085](https://github.com/"
"vllm-project/vllm-ascend/pull/1085)"
msgstr ""
"ä¸€æ‰¹æ€§èƒ½æ”¹è¿›çš„ PR å·²è¢«åˆå¹¶ã€‚[#784](https://github.com/vllm-project/vllm-"
"ascend/pull/784) [#803](https://github.com/vllm-project/vllm-ascend/"
"pull/803) [#966](https://github.com/vllm-project/vllm-ascend/pull/966) "
"[#839](https://github.com/vllm-project/vllm-ascend/pull/839) [#970](https://"
"github.com/vllm-project/vllm-ascend/pull/970) [#947](https://github.com/"
"vllm-project/vllm-ascend/pull/947) [#987](https://github.com/vllm-project/"
"vllm-ascend/pull/987) [#1085](https://github.com/vllm-project/vllm-ascend/"
"pull/1085)"

#: ../../user_guide/release_notes.md:104
msgid ""
"From this release, binary wheel package will be released as well. [#775]"
"(https://github.com/vllm-project/vllm-ascend/pull/775)"
msgstr ""
"ä»æœ¬ç‰ˆæœ¬å¼€å§‹ï¼Œå°†åŒæ—¶å‘å¸ƒäºŒè¿›åˆ¶ wheel åŒ…ã€‚[#775](https://github.com/vllm-"
"project/vllm-ascend/pull/775)"

#: ../../user_guide/release_notes.md:105
msgid ""
"The contributor doc site is [added](https://vllm-ascend.readthedocs.io/en/"
"latest/community/contributors.html)"
msgstr ""
"è´¡çŒ®è€…æ–‡æ¡£ç«™ç‚¹å·²[æ·»åŠ ](https://vllm-ascend.readthedocs.io/en/latest/"
"community/contributors.html)"

#: ../../user_guide/release_notes.md:107
msgid "Known Issue"
msgstr "å·²çŸ¥é—®é¢˜"

#: ../../user_guide/release_notes.md:109
msgid ""
"In some case, vLLM process may be crashed with aclgraph enabled. We're "
"working this issue and it'll be fixed in the next release."
msgstr ""
"åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯ç”¨ aclgraph æ—¶ vLLM è¿›ç¨‹å¯èƒ½ä¼šå´©æºƒã€‚æˆ‘ä»¬æ­£åœ¨å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶"
"å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä¿®å¤ã€‚"

#: ../../user_guide/release_notes.md:110
msgid ""
"Multi node data-parallel doesn't work with this release. This is a known "
"issue in vllm and has been fixed on main branch. [#18981](https://github."
"com/vllm-project/vllm/pull/18981)"
msgstr ""
"å¤šèŠ‚ç‚¹æ•°æ®å¹¶è¡Œåœ¨æ­¤ç‰ˆæœ¬ä¸­æ— æ³•ä½¿ç”¨ã€‚è¿™æ˜¯ vllm ä¸­å·²çŸ¥çš„é—®é¢˜ï¼Œå¹¶å·²åœ¨ä¸»åˆ†æ”¯ä¸­ä¿®"
"å¤ã€‚ [#18981](https://github.com/vllm-project/vllm/pull/18981)"

#: ../../user_guide/release_notes.md:112
msgid "v0.7.3.post1 - 2025.05.29"
msgstr "v0.7.3.post1 - 2025.05.29"

#: ../../user_guide/release_notes.md:114
msgid ""
"This is the first post release of 0.7.3. Please follow the [official doc]"
"(https://vllm-ascend.readthedocs.io/en/v0.7.3-dev) to start the journey. It "
"includes the following changes:"
msgstr ""
"è¿™æ˜¯ 0.7.3 çš„ç¬¬ä¸€ä¸ªè¡¥ä¸å‘å¸ƒã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-ascend."
"readthedocs.io/en/v0.7.3-dev)å¼€å§‹ä½¿ç”¨ã€‚æœ¬æ¬¡æ›´æ–°åŒ…æ‹¬ä»¥ä¸‹æ›´æ”¹ï¼š"

#: ../../user_guide/release_notes.md:118
msgid ""
"Qwen3 and Qwen3MOE is supported now. The performance and accuracy of Qwen3 "
"is well tested. You can try it now. Mindie Turbo is recommended to improve "
"the performance of Qwen3. [#903](https://github.com/vllm-project/vllm-"
"ascend/pull/903) [#915](https://github.com/vllm-project/vllm-ascend/"
"pull/915)"
msgstr ""
"ç°åœ¨å·²æ”¯æŒ Qwen3 å’Œ Qwen3MOEã€‚Qwen3 çš„æ€§èƒ½å’Œç²¾åº¦å·²ç»è¿‡å……åˆ†æµ‹è¯•ï¼Œä½ å¯ä»¥ç«‹å³è¯•"
"ç”¨ã€‚æ¨èä½¿ç”¨ Mindie Turbo ä»¥æå‡ Qwen3 çš„æ€§èƒ½ã€‚[#903](https://github.com/"
"vllm-project/vllm-ascend/pull/903) [#915](https://github.com/vllm-project/"
"vllm-ascend/pull/915)"

#: ../../user_guide/release_notes.md:119
msgid ""
"Added a new performance guide. The guide aims to help users to improve vllm-"
"ascend performance on system level. It includes OS configuration, library "
"optimization, deploy guide and so on. [#878](https://github.com/vllm-"
"project/vllm-ascend/pull/878) [Doc Link](https://vllm-ascend.readthedocs.io/"
"en/v0.7.3-dev/developer_guide/performance/optimization_and_tuning.html)"
msgstr ""
"æ–°å¢äº†ä¸€ä¸ªæ€§èƒ½æŒ‡å—ã€‚è¯¥æŒ‡å—æ—¨åœ¨å¸®åŠ©ç”¨æˆ·åœ¨ç³»ç»Ÿå±‚é¢æå‡ vllm-ascend çš„æ€§èƒ½ã€‚å†…"
"å®¹åŒ…æ‹¬æ“ä½œç³»ç»Ÿé…ç½®ã€åº“ä¼˜åŒ–ã€éƒ¨ç½²æŒ‡å—ç­‰ã€‚ [#878](https://github.com/vllm-"
"project/vllm-ascend/pull/878) [æ–‡æ¡£é“¾æ¥](https://vllm-ascend.readthedocs.io/"
"en/v0.7.3-dev/developer_guide/performance/optimization_and_tuning.html)"

#: ../../user_guide/release_notes.md:121
msgid "Bug Fix"
msgstr "æ¼æ´ä¿®å¤"

#: ../../user_guide/release_notes.md:123
msgid ""
"Qwen2.5-VL  works for RLHF scenarios now. [#928](https://github.com/vllm-"
"project/vllm-ascend/pull/928)"
msgstr ""
"Qwen2.5-VL ç°åœ¨å·²æ”¯æŒ RLHF åœºæ™¯ã€‚[#928](https://github.com/vllm-project/"
"vllm-ascend/pull/928)"

#: ../../user_guide/release_notes.md:124
msgid ""
"Users can launch the model from online weights now. e.g. from huggingface "
"or modelscope directly [#858](https://github.com/vllm-project/vllm-ascend/"
"pull/858) [#918](https://github.com/vllm-project/vllm-ascend/pull/918)"
msgstr ""
"ç”¨æˆ·ç°åœ¨å¯ä»¥ç›´æ¥ä»åœ¨çº¿æƒé‡å¯åŠ¨æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç›´æ¥ä» huggingface æˆ– "
"modelscope è·å–ã€‚[#858](https://github.com/vllm-project/vllm-ascend/"
"pull/858) [#918](https://github.com/vllm-project/vllm-ascend/pull/918)"

#: ../../user_guide/release_notes.md:125
msgid ""
"The meaningless log info `UserWorkspaceSize0` has been cleaned. [#911]"
"(https://github.com/vllm-project/vllm-ascend/pull/911)"
msgstr ""
"æ— æ„ä¹‰çš„æ—¥å¿—ä¿¡æ¯ `UserWorkspaceSize0` å·²è¢«æ¸…ç†ã€‚[#911](https://github.com/"
"vllm-project/vllm-ascend/pull/911)"

#: ../../user_guide/release_notes.md:126
msgid ""
"The log level for `Failed to import vllm_ascend_C` has been changed to "
"`warning` instead of `error`. [#956](https://github.com/vllm-project/vllm-"
"ascend/pull/956)"
msgstr ""
"`Failed to import vllm_ascend_C` çš„æ—¥å¿—çº§åˆ«å·²ä» `error` æ›´æ”¹ä¸º `warning`ã€‚"
"[#956](https://github.com/vllm-project/vllm-ascend/pull/956)"

#: ../../user_guide/release_notes.md:127
msgid ""
"DeepSeek MLA now works with chunked prefill in V1 Engine. Please note that "
"V1 engine in 0.7.3 is just expermential and only for test usage. [#849]"
"(https://github.com/vllm-project/vllm-ascend/pull/849) [#936](https://"
"github.com/vllm-project/vllm-ascend/pull/936)"
msgstr ""
"DeepSeek MLA ç°å·²åœ¨ V1 å¼•æ“ä¸­æ”¯æŒåˆ†å—é¢„å¡«å……ã€‚è¯·æ³¨æ„ï¼Œ0.7.3 ç‰ˆæœ¬ä¸­çš„ V1 å¼•æ“"
"ä»…ä¸ºå®éªŒæ€§ï¼Œä»…ä¾›æµ‹è¯•ä½¿ç”¨ã€‚[#849](https://github.com/vllm-project/vllm-"
"ascend/pull/849) [#936](https://github.com/vllm-project/vllm-ascend/"
"pull/936)"

#: ../../user_guide/release_notes.md:129
msgid "Docs"
msgstr "æ–‡æ¡£"

#: ../../user_guide/release_notes.md:131
msgid ""
"The benchmark doc is updated for Qwen2.5 and Qwen2.5-VL [#792](https://"
"github.com/vllm-project/vllm-ascend/pull/792)"
msgstr ""
"åŸºå‡†æ–‡æ¡£å·²é’ˆå¯¹ Qwen2.5 å’Œ Qwen2.5-VL æ›´æ–° [#792](https://github.com/vllm-"
"project/vllm-ascend/pull/792)"

#: ../../user_guide/release_notes.md:132
msgid ""
"Add the note to clear that only \"modelscope<1.23.0\" works with 0.7.3. "
"[#954](https://github.com/vllm-project/vllm-ascend/pull/954)"
msgstr ""
"æ·»åŠ è¯´æ˜ï¼Œæ˜ç¡®åªæœ‰ \"modelscope<1.23.0\" èƒ½ä¸ 0.7.3 ä¸€èµ·ä½¿ç”¨ã€‚[#954]"
"(https://github.com/vllm-project/vllm-ascend/pull/954)"

#: ../../user_guide/release_notes.md:134
msgid "v0.7.3 - 2025.05.08"
msgstr "v0.7.3 - 2025.05.08"

#: ../../user_guide/release_notes.md:136 ../../user_guide/release_notes.md:277
msgid "ğŸ‰ Hello, World!"
msgstr "ğŸ‰ ä½ å¥½ï¼Œä¸–ç•Œï¼"

#: ../../user_guide/release_notes.md:138
msgid ""
"We are excited to announce the release of 0.7.3 for vllm-ascend. This is "
"the first official release. The functionality, performance, and stability "
"of this release are fully tested and verified. We encourage you to try it "
"out and provide feedback. We'll post bug fix versions in the future if "
"needed. Please follow the [official doc](https://vllm-ascend.readthedocs.io/"
"en/v0.7.3-dev) to start the journey."
msgstr ""
"æˆ‘ä»¬å¾ˆé«˜å…´åœ°å®£å¸ƒ vllm-ascend 0.7.3 ç‰ˆæœ¬æ­£å¼å‘å¸ƒã€‚è¿™æ˜¯é¦–ä¸ªæ­£å¼å‘å¸ƒçš„ç‰ˆæœ¬ã€‚è¯¥"
"ç‰ˆæœ¬çš„åŠŸèƒ½ã€æ€§èƒ½å’Œç¨³å®šæ€§å·²å……åˆ†æµ‹è¯•å’ŒéªŒè¯ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨è¯•ç”¨å¹¶åé¦ˆæ„è§ã€‚å¦‚æœ‰éœ€"
"è¦ï¼Œæœªæ¥æˆ‘ä»¬å°†å‘å¸ƒä¿®å¤ç‰ˆæœ¬ã€‚è¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https://vllm-ascend."
"readthedocs.io/en/v0.7.3-dev)å¼€å¯æ‚¨çš„ä½“éªŒä¹‹æ—…ã€‚"

#: ../../user_guide/release_notes.md:141
msgid ""
"This release includes all features landed in the previous release "
"candidates ([v0.7.1rc1](https://github.com/vllm-project/vllm-ascend/"
"releases/tag/v0.7.1rc1), [v0.7.3rc1](https://github.com/vllm-project/vllm-"
"ascend/releases/tag/v0.7.3rc1), [v0.7.3rc2](https://github.com/vllm-project/"
"vllm-ascend/releases/tag/v0.7.3rc2)). And all the features are fully tested "
"and verified. Visit the official doc the get the detail [feature](https://"
"vllm-ascend.readthedocs.io/en/v0.7.3-dev/user_guide/suppoted_features.html) "
"and [model](https://vllm-ascend.readthedocs.io/en/v0.7.3-dev/user_guide/"
"supported_models.html) support matrix."
msgstr ""
"æœ¬æ¬¡å‘å¸ƒåŒ…å«äº†æ‰€æœ‰åœ¨ä¹‹å‰å€™é€‰ç‰ˆæœ¬ä¸­åŠ å…¥çš„åŠŸèƒ½ï¼ˆ[v0.7.1rc1](https://github."
"com/vllm-project/vllm-ascend/releases/tag/v0.7.1rc1)ã€[v0.7.3rc1](https://"
"github.com/vllm-project/vllm-ascend/releases/tag/v0.7.3rc1)ã€[v0.7.3rc2]"
"(https://github.com/vllm-project/vllm-ascend/releases/tag/v0.7.3rc2)ï¼‰ã€‚æ‰€æœ‰"
"åŠŸèƒ½éƒ½ç»è¿‡äº†å…¨é¢æµ‹è¯•å’ŒéªŒè¯ã€‚è¯·è®¿é—®å®˜æ–¹æ–‡æ¡£è·å–è¯¦ç»†çš„[åŠŸèƒ½](https://vllm-"
"ascend.readthedocs.io/en/v0.7.3-dev/user_guide/suppoted_features.html)å’Œ[æ¨¡"
"å‹](https://vllm-ascend.readthedocs.io/en/v0.7.3-dev/user_guide/"
"supported_models.html)æ”¯æŒçŸ©é˜µã€‚"

#: ../../user_guide/release_notes.md:142
msgid ""
"Upgrade CANN to 8.1.RC1 to enable chunked prefill and automatic prefix "
"caching features. You can now enable them now."
msgstr ""
"å°† CANN å‡çº§åˆ° 8.1.RC1 ä»¥å¯ç”¨åˆ†å—é¢„å¡«å……å’Œè‡ªåŠ¨å‰ç¼€ç¼“å­˜åŠŸèƒ½ã€‚æ‚¨ç°åœ¨å¯ä»¥å¯ç”¨è¿™"
"äº›åŠŸèƒ½äº†ã€‚"

#: ../../user_guide/release_notes.md:143
msgid ""
"Upgrade PyTorch to 2.5.1. vLLM Ascend no longer relies on the dev version "
"of torch-npu now. Now users don't need to install the torch-npu by hand. "
"The 2.5.1 version of torch-npu will be installed automatically. [#662]"
"(https://github.com/vllm-project/vllm-ascend/pull/662)"
msgstr ""
"å‡çº§ PyTorch è‡³ 2.5.1ã€‚vLLM Ascend ç°åœ¨ä¸å†ä¾èµ–äº torch-npu çš„å¼€å‘ç‰ˆæœ¬ã€‚ç”¨æˆ·"
"ç°åœ¨æ— éœ€æ‰‹åŠ¨å®‰è£… torch-npuï¼Œ2.5.1 ç‰ˆæœ¬çš„ torch-npu ä¼šè¢«è‡ªåŠ¨å®‰è£…ã€‚[#662]"
"(https://github.com/vllm-project/vllm-ascend/pull/662)"

#: ../../user_guide/release_notes.md:144
msgid ""
"Integrate MindIE Turbo into vLLM Ascend to improve DeepSeek V3/R1, Qwen 2 "
"series performance. [#708](https://github.com/vllm-project/vllm-ascend/"
"pull/708)"
msgstr ""
"å°† MindIE Turbo é›†æˆåˆ° vLLM Ascend ä»¥æå‡ DeepSeek V3/R1ã€Qwen 2 ç³»åˆ—çš„æ€§"
"èƒ½ã€‚[#708](https://github.com/vllm-project/vllm-ascend/pull/708)"

#: ../../user_guide/release_notes.md:147
msgid ""
"LoRAã€Multi-LoRA And Dynamic Serving is supported now. The performance will "
"be improved in the next release. Please follow the official doc for more "
"usage information. Thanks for the contribution from China Merchants Bank. "
"[#700](https://github.com/vllm-project/vllm-ascend/pull/700)"
msgstr ""
"ç°åœ¨å·²ç»æ”¯æŒ LoRAã€å¤šLoRA å’ŒåŠ¨æ€æœåŠ¡ã€‚ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­æ€§èƒ½å°†ä¼šæå‡ã€‚è¯·å‚é˜…å®˜æ–¹æ–‡"
"æ¡£ä»¥è·å–æ›´å¤šç”¨æ³•ä¿¡æ¯ã€‚æ„Ÿè°¢æ‹›å•†é“¶è¡Œçš„è´¡çŒ®ã€‚[#700](https://github.com/vllm-"
"project/vllm-ascend/pull/700)"

#: ../../user_guide/release_notes.md:150
msgid ""
"The performance of Qwen2 vl and Qwen2.5 vl is improved. [#702](https://"
"github.com/vllm-project/vllm-ascend/pull/702)"
msgstr ""
"Qwen2 vl å’Œ Qwen2.5 vl çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚ [#702](https://github.com/vllm-"
"project/vllm-ascend/pull/702)"

#: ../../user_guide/release_notes.md:151
msgid ""
"The performance of `apply_penalties` and `topKtopP` ops are improved. [#525]"
"(https://github.com/vllm-project/vllm-ascend/pull/525)"
msgstr ""
"`apply_penalties` å’Œ `topKtopP` æ“ä½œçš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚ [#525](https://"
"github.com/vllm-project/vllm-ascend/pull/525)"

#: ../../user_guide/release_notes.md:154
msgid ""
"Fixed a issue that may lead CPU memory leak. [#691](https://github.com/vllm-"
"project/vllm-ascend/pull/691) [#712](https://github.com/vllm-project/vllm-"
"ascend/pull/712)"
msgstr ""
"ä¿®å¤äº†å¯èƒ½å¯¼è‡´CPUå†…å­˜æ³„æ¼çš„é—®é¢˜ã€‚ [#691](https://github.com/vllm-project/"
"vllm-ascend/pull/691) [#712](https://github.com/vllm-project/vllm-ascend/"
"pull/712)"

#: ../../user_guide/release_notes.md:155
msgid ""
"A new environment `SOC_VERSION` is added. If you hit any soc detection "
"error when building with custom ops enabled, please set `SOC_VERSION` to a "
"suitable value. [#606](https://github.com/vllm-project/vllm-ascend/pull/606)"
msgstr ""
"æ–°å¢äº†ä¸€ä¸ªç¯å¢ƒå˜é‡ `SOC_VERSION`ã€‚å¦‚æœåœ¨å¯ç”¨è‡ªå®šä¹‰ç®—å­æ—¶æ„å»ºè¿‡ç¨‹ä¸­é‡åˆ° soc "
"æ£€æµ‹é”™è¯¯ï¼Œè¯·å°† `SOC_VERSION` è®¾ç½®ä¸ºåˆé€‚çš„å€¼ã€‚[#606](https://github.com/vllm-"
"project/vllm-ascend/pull/606)"

#: ../../user_guide/release_notes.md:156
msgid ""
"openEuler container image supported with v0.7.3-openeuler tag. [#665]"
"(https://github.com/vllm-project/vllm-ascend/pull/665)"
msgstr ""
"openEuler å®¹å™¨é•œåƒå·²æ”¯æŒ v0.7.3-openeuler æ ‡ç­¾ã€‚[#665](https://github.com/"
"vllm-project/vllm-ascend/pull/665)"

#: ../../user_guide/release_notes.md:157
msgid ""
"Prefix cache feature works on V1 engine now. [#559](https://github.com/vllm-"
"project/vllm-ascend/pull/559)"
msgstr ""
"å‰ç¼€ç¼“å­˜åŠŸèƒ½ç°åœ¨å·²åœ¨ V1 å¼•æ“ä¸Šå·¥ä½œã€‚[#559](https://github.com/vllm-project/"
"vllm-ascend/pull/559)"

#: ../../user_guide/release_notes.md:159
msgid "v0.8.5rc1 - 2025.05.06"
msgstr "v0.8.5rc1 - 2025.05.06"

#: ../../user_guide/release_notes.md:161
msgid ""
"This is the 1st release candidate of v0.8.5 for vllm-ascend. Please follow "
"the [official doc](https://vllm-ascend.readthedocs.io/en/) to start the "
"journey. Now you can enable V1 egnine by setting the environment variable "
"`VLLM_USE_V1=1`, see the feature support status of vLLM Ascend in [here]"
"(https://vllm-ascend.readthedocs.io/en/latest/user_guide/support_matrix/"
"supported_features.html)."
msgstr ""
"è¿™æ˜¯ vllm-ascend v0.8.5 çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
"ascend.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ "
"`VLLM_USE_V1=1` å¯ç”¨ V1 å¼•æ“ã€‚å…³äº vLLM Ascend çš„ç‰¹æ€§æ”¯æŒæƒ…å†µï¼Œè¯·å‚è§[è¿™é‡Œ]"
"(https://vllm-ascend.readthedocs.io/en/latest/user_guide/support_matrix/"
"supported_features.html)ã€‚"

#: ../../user_guide/release_notes.md:164
msgid ""
"Upgrade CANN version to 8.1.RC1 to support chunked prefill and automatic "
"prefix caching (`--enable_prefix_caching`) when V1 is enabled [#747]"
"(https://github.com/vllm-project/vllm-ascend/pull/747)"
msgstr ""
"å°† CANN ç‰ˆæœ¬å‡çº§åˆ° 8.1.RC1ï¼Œä»¥æ”¯æŒåœ¨å¯ç”¨ V1 æ—¶çš„åˆ†å—é¢„å¡«å……å’Œè‡ªåŠ¨å‰ç¼€ç¼“å­˜"
"ï¼ˆ`--enable_prefix_caching`ï¼‰[#747](https://github.com/vllm-project/vllm-"
"ascend/pull/747)"

#: ../../user_guide/release_notes.md:165
msgid ""
"Optimize Qwen2 VL and Qwen 2.5 VL [#701](https://github.com/vllm-project/"
"vllm-ascend/pull/701)"
msgstr ""
"ä¼˜åŒ– Qwen2 VL å’Œ Qwen 2.5 VL [#701](https://github.com/vllm-project/vllm-"
"ascend/pull/701)"

#: ../../user_guide/release_notes.md:166
#, python-brace-format
msgid ""
"Improve Deepseek V3 eager mode and graph mode performance, now you can use "
"--additional_config={'enable_graph_mode': True} to enable graph mode. [#598]"
"(https://github.com/vllm-project/vllm-ascend/pull/598) [#719](https://"
"github.com/vllm-project/vllm-ascend/pull/719)"
msgstr ""
"æ”¹è¿›äº† Deepseek V3 çš„ eager æ¨¡å¼å’Œå›¾æ¨¡å¼æ€§èƒ½ï¼Œç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ --"
"additional_config={'enable_graph_mode': True} æ¥å¯ç”¨å›¾æ¨¡å¼ã€‚[#598](https://"
"github.com/vllm-project/vllm-ascend/pull/598) [#719](https://github.com/"
"vllm-project/vllm-ascend/pull/719)"

#: ../../user_guide/release_notes.md:169
msgid ""
"Upgrade vLLM to 0.8.5.post1 [#715](https://github.com/vllm-project/vllm-"
"ascend/pull/715)"
msgstr ""
"å°† vLLM å‡çº§åˆ° 0.8.5.post1 [#715](https://github.com/vllm-project/vllm-"
"ascend/pull/715)"

#: ../../user_guide/release_notes.md:170
msgid ""
"Fix early return in CustomDeepseekV2MoE.forward during profile_run [#682]"
"(https://github.com/vllm-project/vllm-ascend/pull/682)"
msgstr ""
"ä¿®å¤åœ¨ profile_run æœŸé—´ CustomDeepseekV2MoE.forward è¿‡æ—©è¿”å›çš„é—®é¢˜ [#682]"
"(https://github.com/vllm-project/vllm-ascend/pull/682)"

#: ../../user_guide/release_notes.md:171
msgid ""
"Adapts for new quant model generated by modelslim [#719](https://github.com/"
"vllm-project/vllm-ascend/pull/719)"
msgstr ""
"é€‚é…ç”± modelslim ç”Ÿæˆçš„æ–°é‡åŒ–æ¨¡å‹ [#719](https://github.com/vllm-project/"
"vllm-ascend/pull/719)"

#: ../../user_guide/release_notes.md:172
msgid ""
"Initial support on P2P Disaggregated Prefill based on llm_datadist [#694]"
"(https://github.com/vllm-project/vllm-ascend/pull/694)"
msgstr ""
"åŸºäº llm_datadist çš„ P2P åˆ†å¸ƒå¼ Prefill åˆæ­¥æ”¯æŒ [#694](https://github.com/"
"vllm-project/vllm-ascend/pull/694)"

#: ../../user_guide/release_notes.md:173
msgid ""
"Use `/vllm-workspace` as code path and include `.git` in container image to "
"fix issue when start vllm under `/workspace` [#726](https://github.com/vllm-"
"project/vllm-ascend/pull/726)"
msgstr ""
"ä½¿ç”¨ `/vllm-workspace` ä½œä¸ºä»£ç è·¯å¾„ï¼Œå¹¶åœ¨å®¹å™¨é•œåƒä¸­åŒ…å« `.git` ï¼Œä»¥ä¿®å¤åœ¨ `/"
"workspace` ä¸‹å¯åŠ¨ vllm æ—¶çš„é—®é¢˜ [#726](https://github.com/vllm-project/vllm-"
"ascend/pull/726)"

#: ../../user_guide/release_notes.md:174
msgid ""
"Optimize NPU memory usage to make DeepSeek R1 W8A8 32K model len work. "
"[#728](https://github.com/vllm-project/vllm-ascend/pull/728)"
msgstr ""
"ä¼˜åŒ–NPUå†…å­˜ä½¿ç”¨ï¼Œä»¥ä½¿ DeepSeek R1 W8A8 32K æ¨¡å‹é•¿åº¦èƒ½å¤Ÿè¿è¡Œã€‚[#728](https://"
"github.com/vllm-project/vllm-ascend/pull/728)"

#: ../../user_guide/release_notes.md:175
msgid ""
"Fix `PYTHON_INCLUDE_PATH` typo in setup.py [#762](https://github.com/vllm-"
"project/vllm-ascend/pull/762)"
msgstr ""
"ä¿®å¤ setup.py ä¸­çš„ `PYTHON_INCLUDE_PATH` æ‹¼å†™é”™è¯¯ [#762](https://github.com/"
"vllm-project/vllm-ascend/pull/762)"

#: ../../user_guide/release_notes.md:178
msgid ""
"Add Qwen3-0.6B test [#717](https://github.com/vllm-project/vllm-ascend/"
"pull/717)"
msgstr ""
"æ·»åŠ  Qwen3-0.6B æµ‹è¯• [#717](https://github.com/vllm-project/vllm-ascend/"
"pull/717)"

#: ../../user_guide/release_notes.md:179
msgid ""
"Add nightly CI [#668](https://github.com/vllm-project/vllm-ascend/pull/668)"
msgstr ""
"æ·»åŠ æ¯æ™šæŒç»­é›†æˆ [#668](https://github.com/vllm-project/vllm-ascend/"
"pull/668)"

#: ../../user_guide/release_notes.md:180
msgid ""
"Add accuracy test report [#542](https://github.com/vllm-project/vllm-ascend/"
"pull/542)"
msgstr ""
"æ·»åŠ å‡†ç¡®æ€§æµ‹è¯•æŠ¥å‘Š [#542](https://github.com/vllm-project/vllm-ascend/"
"pull/542)"

#: ../../user_guide/release_notes.md:182
msgid "v0.8.4rc2 - 2025.04.29"
msgstr "v0.8.4rc2 - 2025.04.29"

#: ../../user_guide/release_notes.md:184
msgid ""
"This is the second release candidate of v0.8.4 for vllm-ascend. Please "
"follow the [official doc](https://vllm-ascend.readthedocs.io/en/) to start "
"the journey. Some experimental features are included in this version, such "
"as W8A8 quantization and EP/DP support. We'll make them stable enough in "
"the next release."
msgstr ""
"è¿™æ˜¯ vllm-ascend çš„ v0.8.4 ç¬¬äºŒä¸ªå€™é€‰ç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
"ascend.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚æœ¬ç‰ˆæœ¬åŒ…å«äº†ä¸€äº›å®éªŒæ€§åŠŸèƒ½ï¼Œå¦‚ W8A8 é‡åŒ–"
"å’Œ EP/DP æ”¯æŒã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä½¿è¿™äº›åŠŸèƒ½æ›´åŠ ç¨³å®šã€‚"

#: ../../user_guide/release_notes.md:187
msgid ""
"Qwen3 and Qwen3MOE is supported now. Please follow the [official doc]"
"(https://vllm-ascend.readthedocs.io/en/latest/tutorials/single_npu.html) to "
"run the quick demo. [#709](https://github.com/vllm-project/vllm-ascend/"
"pull/709)"
msgstr ""
"ç°åœ¨å·²æ”¯æŒ Qwen3 å’Œ Qwen3MOEã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-ascend."
"readthedocs.io/en/latest/tutorials/single_npu.html)è¿è¡Œå¿«é€Ÿæ¼”ç¤ºã€‚[#709]"
"(https://github.com/vllm-project/vllm-ascend/pull/709)"

#: ../../user_guide/release_notes.md:188
msgid ""
"Ascend W8A8 quantization method is supported now. Please take the [official "
"doc](https://vllm-ascend.readthedocs.io/en/latest/tutorials/"
"multi_npu_quantization.html) for example. Any [feedback](https://github.com/"
"vllm-project/vllm-ascend/issues/619) is welcome. [#580](https://github.com/"
"vllm-project/vllm-ascend/pull/580)"
msgstr ""
"ç°åœ¨æ”¯æŒ Ascend W8A8 é‡åŒ–æ–¹æ³•ã€‚è¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://vllm-ascend."
"readthedocs.io/en/latest/tutorials/multi_npu_quantization.html) ç¤ºä¾‹ã€‚æ¬¢è¿æ"
"ä¾›ä»»ä½•[åé¦ˆ](https://github.com/vllm-project/vllm-ascend/issues/619)ã€‚[#580]"
"(https://github.com/vllm-project/vllm-ascend/pull/580)"

#: ../../user_guide/release_notes.md:189
msgid ""
"DeepSeek V3/R1 works with DP, TP and MTP now. Please note that it's still "
"in experimental status. Let us know if you hit any problem. [#429](https://"
"github.com/vllm-project/vllm-ascend/pull/429) [#585](https://github.com/"
"vllm-project/vllm-ascend/pull/585)  [#626](https://github.com/vllm-project/"
"vllm-ascend/pull/626) [#636](https://github.com/vllm-project/vllm-ascend/"
"pull/636) [#671](https://github.com/vllm-project/vllm-ascend/pull/671)"
msgstr ""
"DeepSeek V3/R1 ç°åœ¨å·²ç»æ”¯æŒ DPã€TP å’Œ MTPã€‚è¯·æ³¨æ„ï¼Œç›®å‰ä»å¤„äºå®éªŒé˜¶æ®µã€‚å¦‚æœ"
"é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·å‘ŠçŸ¥æˆ‘ä»¬ã€‚ [#429](https://github.com/vllm-project/vllm-"
"ascend/pull/429) [#585](https://github.com/vllm-project/vllm-ascend/"
"pull/585) [#626](https://github.com/vllm-project/vllm-ascend/pull/626) "
"[#636](https://github.com/vllm-project/vllm-ascend/pull/636) [#671](https://"
"github.com/vllm-project/vllm-ascend/pull/671)"

#: ../../user_guide/release_notes.md:192
msgid ""
"ACLGraph feature is supported with V1 engine now. It's disabled by default "
"because this feature rely on CANN 8.1 release. We'll make it available by "
"default in the next release [#426](https://github.com/vllm-project/vllm-"
"ascend/pull/426)"
msgstr ""
"ACLGraph ç‰¹æ€§ç°åœ¨å·²è¢« V1 å¼•æ“æ”¯æŒã€‚å®ƒé»˜è®¤æ˜¯ç¦ç”¨çš„ï¼Œå› ä¸ºè¯¥ç‰¹æ€§ä¾èµ–äº CANN "
"8.1 ç‰ˆæœ¬ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­é»˜è®¤å¯ç”¨æ­¤ç‰¹æ€§ [#426](https://github.com/vllm-"
"project/vllm-ascend/pull/426)ã€‚"

#: ../../user_guide/release_notes.md:193
msgid ""
"Upgrade PyTorch to 2.5.1. vLLM Ascend no longer relies on the dev version "
"of torch-npu now. Now users don't need to install the torch-npu by hand. "
"The 2.5.1 version of torch-npu will be installed automatically. [#661]"
"(https://github.com/vllm-project/vllm-ascend/pull/661)"
msgstr ""
"å‡çº§ PyTorch è‡³ 2.5.1ã€‚vLLM Ascend ç°åœ¨ä¸å†ä¾èµ– dev ç‰ˆæœ¬çš„ torch-npuï¼Œç”¨æˆ·æ— "
"éœ€æ‰‹åŠ¨å®‰è£… torch-npuã€‚torch-npu çš„ 2.5.1 ç‰ˆæœ¬å°†ä¼šè‡ªåŠ¨å®‰è£…ã€‚[#661](https://"
"github.com/vllm-project/vllm-ascend/pull/661)"

#: ../../user_guide/release_notes.md:196
msgid ""
"MiniCPM model works now. [#645](https://github.com/vllm-project/vllm-ascend/"
"pull/645)"
msgstr ""
"MiniCPM æ¨¡å‹ç°åœ¨å¯ä»¥ä½¿ç”¨äº†ã€‚[#645](https://github.com/vllm-project/vllm-"
"ascend/pull/645)"

#: ../../user_guide/release_notes.md:197
msgid ""
"openEuler container image supported with `v0.8.4-openeuler` tag and customs "
"Ops build is enabled by default for openEuler OS. [#689](https://github.com/"
"vllm-project/vllm-ascend/pull/689)"
msgstr ""
"openEuler å®¹å™¨é•œåƒå·²æ”¯æŒ `v0.8.4-openeuler` æ ‡ç­¾ï¼Œå¹¶ä¸” openEuler æ“ä½œç³»ç»Ÿé»˜"
"è®¤å¯ç”¨äº†è‡ªå®šä¹‰ Ops æ„å»ºã€‚[#689](https://github.com/vllm-project/vllm-ascend/"
"pull/689)"

#: ../../user_guide/release_notes.md:198
msgid ""
"Fix ModuleNotFoundError bug to make Lora work [#600](https://github.com/"
"vllm-project/vllm-ascend/pull/600)"
msgstr ""
"ä¿®å¤ ModuleNotFoundError é”™è¯¯ä»¥ä½¿ Lora æ­£å¸¸å·¥ä½œ [#600](https://github.com/"
"vllm-project/vllm-ascend/pull/600)"

#: ../../user_guide/release_notes.md:199
msgid ""
"Add \"Using EvalScope evaluation\" doc [#611](https://github.com/vllm-"
"project/vllm-ascend/pull/611)"
msgstr ""
"æ·»åŠ äº†â€œä½¿ç”¨ EvalScope è¯„ä¼°â€æ–‡æ¡£ [#611](https://github.com/vllm-project/vllm-"
"ascend/pull/611)"

#: ../../user_guide/release_notes.md:200
msgid ""
"Add a `VLLM_VERSION` environment to make vLLM version configurable to help "
"developer set correct vLLM version if the code of vLLM is changed by hand "
"locally. [#651](https://github.com/vllm-project/vllm-ascend/pull/651)"
msgstr ""
"æ–°å¢äº†ä¸€ä¸ª `VLLM_VERSION` ç¯å¢ƒå˜é‡ï¼Œä½¿ vLLM ç‰ˆæœ¬å¯ä»¥é…ç½®ï¼Œå¸®åŠ©å¼€å‘è€…åœ¨æœ¬åœ°æ‰‹"
"åŠ¨ä¿®æ”¹ vLLM ä»£ç åï¼Œè®¾ç½®æ­£ç¡®çš„ vLLM ç‰ˆæœ¬ã€‚[#651](https://github.com/vllm-"
"project/vllm-ascend/pull/651)"

#: ../../user_guide/release_notes.md:202
msgid "v0.8.4rc1 - 2025.04.18"
msgstr "v0.8.4rc1 - 2025.04.18"

#: ../../user_guide/release_notes.md:204
msgid ""
"This is the first release candidate of v0.8.4 for vllm-ascend. Please "
"follow the [official doc](https://vllm-ascend.readthedocs.io/en/) to start "
"the journey. From this version, vllm-ascend will follow the newest version "
"of vllm and release every two weeks. For example, if vllm releases v0.8.5 "
"in the next two weeks, vllm-ascend will release v0.8.5rc1 instead of "
"v0.8.4rc2. Please find the detail from the [official documentation](https://"
"vllm-ascend.readthedocs.io/en/latest/community/versioning_policy."
"html#release-window)."
msgstr ""
"è¿™æ˜¯ vllm-ascend v0.8.4 çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
"ascend.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚æœ¬ç‰ˆæœ¬èµ·ï¼Œvllm-ascend å°†è·Ÿéš vllm çš„æœ€æ–°"
"ç‰ˆæœ¬å¹¶æ¯ä¸¤å‘¨å‘å¸ƒä¸€æ¬¡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ vllm åœ¨æ¥ä¸‹æ¥çš„ä¸¤å‘¨å†…å‘å¸ƒ v0.8.5ï¼Œvllm-"
"ascend å°†å‘å¸ƒ v0.8.5rc1ï¼Œè€Œä¸æ˜¯ v0.8.4rc2ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://"
"vllm-ascend.readthedocs.io/en/latest/community/versioning_policy."
"html#release-window)ã€‚"

#: ../../user_guide/release_notes.md:208
msgid ""
"vLLM V1 engine experimental support is included in this version. You can "
"visit [official guide](https://docs.vllm.ai/en/latest/getting_started/"
"v1_user_guide.html) to get more detail. By default, vLLM will fallback to "
"V0 if V1 doesn't work, please set `VLLM_USE_V1=1` environment if you want "
"to use V1 forcibly."
msgstr ""
"æœ¬ç‰ˆæœ¬åŒ…å«äº†å¯¹ vLLM V1 å¼•æ“çš„å®éªŒæ€§æ”¯æŒã€‚ä½ å¯ä»¥è®¿é—®[å®˜æ–¹æŒ‡å—](https://docs."
"vllm.ai/en/latest/getting_started/v1_user_guide.html)è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚é»˜è®¤"
"æƒ…å†µä¸‹ï¼Œå¦‚æœ V1 ä¸å¯ç”¨ï¼ŒvLLM ä¼šè‡ªåŠ¨å›é€€åˆ° V0ã€‚å¦‚æœä½ æƒ³å¼ºåˆ¶ä½¿ç”¨ V1ï¼Œè¯·è®¾ç½® "
"`VLLM_USE_V1=1` ç¯å¢ƒå˜é‡ã€‚"

#: ../../user_guide/release_notes.md:209
msgid ""
"LoRAã€Multi-LoRA And Dynamic Serving is supported now. The performance will "
"be improved in the next release. Please follow the [official doc](https://"
"docs.vllm.ai/en/latest/features/lora.html) for more usage information. "
"Thanks for the contribution from China Merchants Bank. [#521](https://"
"github.com/vllm-project/vllm-ascend/pull/521)."
msgstr ""
"ç°åœ¨å·²æ”¯æŒ LoRAã€Multi-LoRA å’ŒåŠ¨æ€æœåŠ¡ã€‚æ€§èƒ½å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­å¾—åˆ°æå‡ã€‚è¯·å‚é˜…"
"[å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/en/latest/features/lora.html)è·å–æ›´å¤šä½¿ç”¨ä¿¡"
"æ¯ã€‚æ„Ÿè°¢æ‹›å•†é“¶è¡Œçš„è´¡çŒ®ã€‚[#521](https://github.com/vllm-project/vllm-ascend/"
"pull/521)ã€‚"

#: ../../user_guide/release_notes.md:210
msgid ""
"Sleep Mode feature is supported. Currently it's only work on V0 engine. V1 "
"engine support will come soon. [#513](https://github.com/vllm-project/vllm-"
"ascend/pull/513)"
msgstr ""
"å·²æ”¯æŒä¼‘çœ æ¨¡å¼åŠŸèƒ½ã€‚ç›®å‰å®ƒåªåœ¨V0å¼•æ“ä¸Šæœ‰æ•ˆï¼ŒV1å¼•æ“çš„æ”¯æŒå³å°†åˆ°æ¥ã€‚[#513]"
"(https://github.com/vllm-project/vllm-ascend/pull/513)"

#: ../../user_guide/release_notes.md:214
msgid ""
"The Ascend scheduler is added for V1 engine. This scheduler is more "
"affinity with Ascend hardware. More scheduler policy will be added in the "
"future. [#543](https://github.com/vllm-project/vllm-ascend/pull/543)"
msgstr ""
"ä¸ºV1å¼•æ“æ–°å¢äº†Ascendè°ƒåº¦å™¨ã€‚è¯¥è°ƒåº¦å™¨ä¸Ascendç¡¬ä»¶æ›´åŠ é€‚é…ã€‚æœªæ¥è¿˜å°†æ·»åŠ æ›´å¤šè°ƒ"
"åº¦ç­–ç•¥ã€‚ [#543](https://github.com/vllm-project/vllm-ascend/pull/543)"

#: ../../user_guide/release_notes.md:215
msgid ""
"Disaggregated Prefill feature is supported. Currently only 1P1D works. NPND "
"is under design by vllm team. vllm-ascend will support it once it's ready "
"from vLLM. Follow the [official guide](https://docs.vllm.ai/en/latest/"
"features/disagg_prefill.html) to use. [#432](https://github.com/vllm-"
"project/vllm-ascend/pull/432)"
msgstr ""
"æ”¯æŒåˆ†ç¦»å¼é¢„å¡«å……ï¼ˆDisaggregated Prefillï¼‰åŠŸèƒ½ã€‚ç›®å‰ä»…æ”¯æŒ1P1Dï¼ŒNPNDæ­£åœ¨ç”±"
"vllmå›¢é˜Ÿè®¾è®¡ä¸­ã€‚ä¸€æ—¦vLLMæ”¯æŒï¼Œvllm-ascendå°†ä¼šæ”¯æŒã€‚è¯·æŒ‰ç…§[å®˜æ–¹æŒ‡å—](https://"
"docs.vllm.ai/en/latest/features/disagg_prefill.html)ä½¿ç”¨ã€‚[#432](https://"
"github.com/vllm-project/vllm-ascend/pull/432)"

#: ../../user_guide/release_notes.md:216
msgid ""
"Spec decode feature works now. Currently it's only work on V0 engine. V1 "
"engine support will come soon. [#500](https://github.com/vllm-project/vllm-"
"ascend/pull/500)"
msgstr ""
"Spec è§£ç åŠŸèƒ½ç°åœ¨å¯ä»¥ä½¿ç”¨ã€‚ç›®å‰å®ƒåªåœ¨ V0 å¼•æ“ä¸Šå·¥ä½œï¼Œå¯¹ V1 å¼•æ“çš„æ”¯æŒå³å°†åˆ°"
"æ¥ã€‚[#500](https://github.com/vllm-project/vllm-ascend/pull/500)"

#: ../../user_guide/release_notes.md:217
msgid ""
"Structured output feature works now on V1 Engine. Currently it only "
"supports xgrammar backend while using guidance backend may get some errors. "
"[#555](https://github.com/vllm-project/vllm-ascend/pull/555)"
msgstr ""
"ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½ç°åœ¨å·²åœ¨V1å¼•æ“ä¸Šç”Ÿæ•ˆã€‚ç›®å‰ä»…æ”¯æŒxgrammaråç«¯ï¼Œä½¿ç”¨guidanceåç«¯"
"å¯èƒ½ä¼šå‡ºç°ä¸€äº›é”™è¯¯ã€‚[#555](https://github.com/vllm-project/vllm-ascend/"
"pull/555)"

#: ../../user_guide/release_notes.md:221
msgid ""
"A new communicator `pyhccl` is added. It's used for call CANN HCCL library "
"directly instead of using `torch.distribute`. More usage of it will be "
"added in the next release [#503](https://github.com/vllm-project/vllm-"
"ascend/pull/503)"
msgstr ""
"æ–°å¢äº†ä¸€ä¸ªé€šä¿¡å™¨ `pyhccl`ã€‚å®ƒç”¨äºç›´æ¥è°ƒç”¨ CANN HCCL åº“ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ `torch."
"distribute`ã€‚å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­æ·»åŠ æ›´å¤šç”¨æ³• [#503](https://github.com/vllm-"
"project/vllm-ascend/pull/503)ã€‚"

#: ../../user_guide/release_notes.md:222
msgid ""
"The custom ops build is enabled by default. You should install the packages "
"like `gcc`, `cmake` first to build `vllm-ascend` from source. Set "
"`COMPILE_CUSTOM_KERNELS=0` environment to disable the compilation if you "
"don't need it. [#466](https://github.com/vllm-project/vllm-ascend/pull/466)"
msgstr ""
"è‡ªå®šä¹‰ç®—å­çš„æ„å»ºé»˜è®¤æ˜¯å¯ç”¨çš„ã€‚ä½ åº”è¯¥å…ˆå®‰è£…å¦‚ `gcc`ã€`cmake` ç­‰åŒ…ä»¥ä¾¿ä»æºç ç¼–"
"è¯‘ `vllm-ascend`ã€‚å¦‚æœä¸éœ€è¦è‡ªå®šä¹‰ç®—å­çš„ç¼–è¯‘ï¼Œå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ "
"`COMPILE_CUSTOM_KERNELS=0` æ¥ç¦ç”¨ç¼–è¯‘ã€‚ [#466](https://github.com/vllm-"
"project/vllm-ascend/pull/466)"

#: ../../user_guide/release_notes.md:223
msgid ""
"The custom op `rotay embedding` is enabled by default now to improve the "
"performance. [#555](https://github.com/vllm-project/vllm-ascend/pull/555)"
msgstr ""
"è‡ªå®šä¹‰ç®—å­ `rotay embedding` ç°åœ¨å·²é»˜è®¤å¯ç”¨ï¼Œä»¥æå‡æ€§èƒ½ã€‚[#555](https://"
"github.com/vllm-project/vllm-ascend/pull/555)"

#: ../../user_guide/release_notes.md:225
msgid "v0.7.3rc2 - 2025.03.29"
msgstr "v0.7.3rc2 - 2025.03.29"

#: ../../user_guide/release_notes.md:227
msgid ""
"This is 2nd release candidate of v0.7.3 for vllm-ascend. Please follow the "
"[official doc](https://vllm-ascend.readthedocs.io/en/v0.7.3-dev) to start "
"the journey."
msgstr ""
"è¿™æ˜¯ vllm-ascend v0.7.3 çš„ç¬¬äºŒä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://vllm-"
"ascend.readthedocs.io/en/v0.7.3-dev)å¼€å§‹ä½¿ç”¨ã€‚"

#: ../../user_guide/release_notes.md:228 ../../user_guide/release_notes.md:250
msgid ""
"Quickstart with container: https://vllm-ascend.readthedocs.io/en/v0.7.3-dev/"
"quick_start.html"
msgstr ""
"å®¹å™¨å¿«é€Ÿå…¥é—¨ï¼š https://vllm-ascend.readthedocs.io/en/v0.7.3-dev/quick_start."
"html"

#: ../../user_guide/release_notes.md:229 ../../user_guide/release_notes.md:251
msgid ""
"Installation: https://vllm-ascend.readthedocs.io/en/v0.7.3-dev/installation."
"html"
msgstr ""
"å®‰è£…: https://vllm-ascend.readthedocs.io/en/v0.7.3-dev/installation.html"

#: ../../user_guide/release_notes.md:232
msgid ""
"Add Ascend Custom Ops framework. Developers now can write customs ops using "
"AscendC. An example ops `rotary_embedding` is added. More tutorials will "
"come soon. The Custom Ops compilation is disabled by default when "
"installing vllm-ascend. Set `COMPILE_CUSTOM_KERNELS=1` to enable it.  [#371]"
"(https://github.com/vllm-project/vllm-ascend/pull/371)"
msgstr ""
"æ–°å¢äº†Ascendè‡ªå®šä¹‰ç®—å­æ¡†æ¶ã€‚å¼€å‘è€…ç°åœ¨å¯ä»¥ä½¿ç”¨AscendCç¼–å†™è‡ªå®šä¹‰ç®—å­ã€‚æ–°å¢äº†"
"ä¸€ä¸ªç¤ºä¾‹ç®—å­ `rotary_embedding` ã€‚æ›´å¤šæ•™ç¨‹å³å°†å‘å¸ƒã€‚å®‰è£…vllm-ascendæ—¶ï¼Œè‡ªå®š"
"ä¹‰ç®—å­çš„ç¼–è¯‘é»˜è®¤æ˜¯å…³é—­çš„ã€‚å¯é€šè¿‡è®¾ç½® `COMPILE_CUSTOM_KERNELS=1` å¯ç”¨ã€‚[#371]"
"(https://github.com/vllm-project/vllm-ascend/pull/371)"

#: ../../user_guide/release_notes.md:233
msgid ""
"V1 engine is basic supported in this release. The full support will be done "
"in 0.8.X release. If you hit any issue or have any requirement of V1 "
"engine. Please tell us [here](https://github.com/vllm-project/vllm-ascend/"
"issues/414). [#376](https://github.com/vllm-project/vllm-ascend/pull/376)"
msgstr ""
"æœ¬ç‰ˆæœ¬å¯¹ V1 å¼•æ“æä¾›äº†åŸºç¡€æ”¯æŒï¼Œå…¨é¢æ”¯æŒå°†åœ¨ 0.8.X ç‰ˆæœ¬ä¸­å®Œæˆã€‚å¦‚æœæ‚¨é‡åˆ°ä»»"
"ä½•é—®é¢˜æˆ–æœ‰ V1 å¼•æ“çš„ç›¸å…³éœ€æ±‚ï¼Œè¯·åœ¨[è¿™é‡Œ](https://github.com/vllm-project/"
"vllm-ascend/issues/414)å‘Šè¯‰æˆ‘ä»¬ã€‚[#376](https://github.com/vllm-project/"
"vllm-ascend/pull/376)"

#: ../../user_guide/release_notes.md:234
msgid ""
"Prefix cache feature works now. You can set `enable_prefix_caching=True` to "
"enable it. [#282](https://github.com/vllm-project/vllm-ascend/pull/282)"
msgstr ""
"å‰ç¼€ç¼“å­˜åŠŸèƒ½ç°åœ¨å·²ç»å¯ç”¨ã€‚ä½ å¯ä»¥é€šè¿‡è®¾ç½® `enable_prefix_caching=True` æ¥å¯ç”¨"
"è¯¥åŠŸèƒ½ã€‚[#282](https://github.com/vllm-project/vllm-ascend/pull/282)"

#: ../../user_guide/release_notes.md:237
msgid ""
"Bump torch_npu version to dev20250320.3 to improve accuracy to fix `!!!` "
"output problem. [#406](https://github.com/vllm-project/vllm-ascend/pull/406)"
msgstr ""
"å°† torch_npu ç‰ˆæœ¬å‡çº§åˆ° dev20250320.3 ä»¥æå‡ç²¾åº¦ï¼Œä¿®å¤ `!!!` è¾“å‡ºé—®é¢˜ã€‚"
"[#406](https://github.com/vllm-project/vllm-ascend/pull/406)"

#: ../../user_guide/release_notes.md:240
msgid ""
"The performance of Qwen2-vl is improved by optimizing patch embedding "
"(Conv3D). [#398](https://github.com/vllm-project/vllm-ascend/pull/398)"
msgstr ""
"é€šè¿‡ä¼˜åŒ– patch embeddingï¼ˆConv3Dï¼‰ï¼ŒQwen2-vl çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚[#398]"
"(https://github.com/vllm-project/vllm-ascend/pull/398)"

#: ../../user_guide/release_notes.md:244
msgid ""
"Fixed a bug to make sure multi step scheduler feature work. [#349](https://"
"github.com/vllm-project/vllm-ascend/pull/349)"
msgstr ""
"ä¿®å¤äº†ä¸€ä¸ªé”™è¯¯ï¼Œä»¥ç¡®ä¿å¤šæ­¥è°ƒåº¦å™¨åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚[#349](https://github.com/"
"vllm-project/vllm-ascend/pull/349)"

#: ../../user_guide/release_notes.md:245
msgid ""
"Fixed a bug to make prefix cache feature works with correct accuracy. [#424]"
"(https://github.com/vllm-project/vllm-ascend/pull/424)"
msgstr ""
"ä¿®å¤äº†ä¸€ä¸ª bugï¼Œä½¿å‰ç¼€ç¼“å­˜åŠŸèƒ½èƒ½å¤Ÿä»¥æ­£ç¡®çš„å‡†ç¡®æ€§è¿è¡Œã€‚[#424](https://github."
"com/vllm-project/vllm-ascend/pull/424)"

#: ../../user_guide/release_notes.md:247
msgid "v0.7.3rc1 - 2025.03.14"
msgstr "v0.7.3rc1 - 2025.03.14"

#: ../../user_guide/release_notes.md:249
msgid ""
"ğŸ‰ Hello, World! This is the first release candidate of v0.7.3 for vllm-"
"ascend. Please follow the [official doc](https://vllm-ascend.readthedocs.io/"
"en/v0.7.3-dev) to start the journey."
msgstr ""
"ğŸ‰ ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ vllm-ascend v0.7.3 çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡"
"æ¡£](https://vllm-ascend.readthedocs.io/en/v0.7.3-dev)å¼€å§‹ä½ çš„æ—…ç¨‹ã€‚"

#: ../../user_guide/release_notes.md:254
msgid ""
"DeepSeek V3/R1 works well now. Read the [official guide](https://vllm-"
"ascend.readthedocs.io/en/v0.7.3-dev/tutorials/multi_node.html) to start! "
"[#242](https://github.com/vllm-project/vllm-ascend/pull/242)"
msgstr ""
"DeepSeek V3/R1 ç°åœ¨è¿è¡Œè‰¯å¥½ã€‚è¯·é˜…è¯»[å®˜æ–¹æŒ‡å—](https://vllm-ascend."
"readthedocs.io/en/v0.7.3-dev/tutorials/multi_node.html)å¼€å§‹ï¼[#242](https://"
"github.com/vllm-project/vllm-ascend/pull/242)"

#: ../../user_guide/release_notes.md:255
msgid ""
"Speculative decoding feature is supported. [#252](https://github.com/vllm-"
"project/vllm-ascend/pull/252)"
msgstr ""
"å·²æ”¯æŒçŒœæµ‹æ€§è§£ç åŠŸèƒ½ã€‚[#252](https://github.com/vllm-project/vllm-ascend/"
"pull/252)"

#: ../../user_guide/release_notes.md:256
msgid ""
"Multi step scheduler feature is supported. [#300](https://github.com/vllm-"
"project/vllm-ascend/pull/300)"
msgstr ""
"å·²æ”¯æŒå¤šæ­¥è°ƒåº¦å™¨åŠŸèƒ½ã€‚[#300](https://github.com/vllm-project/vllm-ascend/"
"pull/300)"

#: ../../user_guide/release_notes.md:259
msgid ""
"Bump torch_npu version to dev20250308.3 to improve `_exponential` accuracy"
msgstr "å°† torch_npu ç‰ˆæœ¬å‡çº§åˆ° dev20250308.3ï¼Œä»¥æå‡ `_exponential` çš„ç²¾åº¦"

#: ../../user_guide/release_notes.md:260
msgid ""
"Added initial support for pooling models. Bert based model, such as `BAAI/"
"bge-base-en-v1.5` and `BAAI/bge-reranker-v2-m3` works now. [#229](https://"
"github.com/vllm-project/vllm-ascend/pull/229)"
msgstr ""
"æ–°å¢äº†å¯¹æ± åŒ–æ¨¡å‹çš„åˆæ­¥æ”¯æŒã€‚ç°åœ¨æ”¯æŒ Bert åŸºç¡€æ¨¡å‹ï¼Œå¦‚ `BAAI/bge-base-en-"
"v1.5` å’Œ `BAAI/bge-reranker-v2-m3`ã€‚ [#229](https://github.com/vllm-project/"
"vllm-ascend/pull/229)"

#: ../../user_guide/release_notes.md:263
msgid ""
"The performance of Qwen2-VL is improved. [#241](https://github.com/vllm-"
"project/vllm-ascend/pull/241)"
msgstr ""
"Qwen2-VL çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚[#241](https://github.com/vllm-project/vllm-"
"ascend/pull/241)"

#: ../../user_guide/release_notes.md:264
msgid ""
"MiniCPM is now supported [#164](https://github.com/vllm-project/vllm-ascend/"
"pull/164)"
msgstr ""
"MiniCPM ç°åœ¨å·²è¢«æ”¯æŒ [#164](https://github.com/vllm-project/vllm-ascend/"
"pull/164)"

#: ../../user_guide/release_notes.md:267
msgid ""
"Support MTP(Multi-Token Prediction) for DeepSeek V3/R1 [#236](https://"
"github.com/vllm-project/vllm-ascend/pull/236)"
msgstr ""
"ä¸º DeepSeek V3/R1 æ”¯æŒ MTPï¼ˆå¤šæ ‡è®°é¢„æµ‹ï¼‰ [#236](https://github.com/vllm-"
"project/vllm-ascend/pull/236)"

#: ../../user_guide/release_notes.md:268
msgid ""
"[Docs] Added more model tutorials, include DeepSeek, QwQ, Qwen and Qwen "
"2.5VL. See the [official doc](https://vllm-ascend.readthedocs.io/en/v0.7.3-"
"dev/tutorials/index.html) for detail"
msgstr ""
"[æ–‡æ¡£] å¢åŠ äº†æ›´å¤šçš„æ¨¡å‹æ•™ç¨‹ï¼ŒåŒ…æ‹¬ DeepSeekã€QwQã€Qwen å’Œ Qwen 2.5VLã€‚è¯¦æƒ…è¯·"
"å‚è§[å®˜æ–¹æ–‡æ¡£](https://vllm-ascend.readthedocs.io/en/v0.7.3-dev/tutorials/"
"index.html)ã€‚"

#: ../../user_guide/release_notes.md:269
msgid ""
"Pin modelscope<1.23.0 on vLLM v0.7.3 to resolve: https://github.com/vllm-"
"project/vllm/pull/13807"
msgstr ""
"åœ¨ vLLM v0.7.3 ä¸Šé”å®š modelscope ç‰ˆæœ¬ä½äº 1.23.0ï¼Œä»¥è§£å†³ï¼šhttps://github."
"com/vllm-project/vllm/pull/13807"

#: ../../user_guide/release_notes.md:271 ../../user_guide/release_notes.md:302
msgid "Known issues"
msgstr "å·²çŸ¥é—®é¢˜"

#: ../../user_guide/release_notes.md:272
msgid ""
"In [some cases](https://github.com/vllm-project/vllm-ascend/issues/324), "
"especially when the input/output is very long, the accuracy of output may "
"be incorrect. We are working on it. It'll be fixed in the next release."
msgstr ""
"åœ¨[æŸäº›æƒ…å†µä¸‹](https://github.com/vllm-project/vllm-ascend/issues/324)ï¼Œç‰¹åˆ«"
"æ˜¯å½“è¾“å…¥æˆ–è¾“å‡ºéå¸¸é•¿æ—¶ï¼Œè¾“å‡ºçš„å‡†ç¡®æ€§å¯èƒ½ä¼šæœ‰è¯¯ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å°†åœ¨ä¸‹"
"ä¸€ä¸ªç‰ˆæœ¬ä¸­ä¿®å¤ã€‚"

#: ../../user_guide/release_notes.md:273
msgid ""
"Improved and reduced the garbled code in model output. But if you still hit "
"the issue, try to change the generation config value, such as "
"`temperature`, and try again. There is also a known issue shown below. Any "
"[feedback](https://github.com/vllm-project/vllm-ascend/issues/267) is "
"welcome. [#277](https://github.com/vllm-project/vllm-ascend/pull/277)"
msgstr ""
"æ”¹è¿›å¹¶å‡å°‘äº†æ¨¡å‹è¾“å‡ºä¸­çš„ä¹±ç é—®é¢˜ã€‚ä½†å¦‚æœä½ ä»ç„¶é‡åˆ°è¯¥é—®é¢˜ï¼Œè¯·å°è¯•æ›´æ”¹ç”Ÿæˆé…ç½®"
"çš„å‚æ•°ï¼Œä¾‹å¦‚ `temperature`ï¼Œç„¶åå†è¯•ä¸€æ¬¡ã€‚ä¸‹é¢è¿˜åˆ—å‡ºäº†ä¸€ä¸ªå·²çŸ¥é—®é¢˜ã€‚æ¬¢è¿æä¾›"
"ä»»ä½•[åé¦ˆ](https://github.com/vllm-project/vllm-ascend/issues/267)ã€‚[#277]"
"(https://github.com/vllm-project/vllm-ascend/pull/277)"

#: ../../user_guide/release_notes.md:275
msgid "v0.7.1rc1 - 2025.02.19"
msgstr "v0.7.1rc1 - 2025.02.19"

#: ../../user_guide/release_notes.md:279
msgid ""
"We are excited to announce the first release candidate of v0.7.1 for vllm-"
"ascend."
msgstr "æˆ‘ä»¬å¾ˆé«˜å…´åœ°å®£å¸ƒ vllm-ascend v0.7.1 çš„ç¬¬ä¸€ä¸ªå€™é€‰ç‰ˆæœ¬å‘å¸ƒã€‚"

#: ../../user_guide/release_notes.md:281
msgid ""
"vLLM Ascend Plugin (vllm-ascend) is a community maintained hardware plugin "
"for running vLLM on the Ascend NPU. With this release, users can now enjoy "
"the latest features and improvements of vLLM on the Ascend NPU."
msgstr ""
"vLLM Ascend æ’ä»¶ï¼ˆvllm-ascendï¼‰æ˜¯ä¸€ä¸ªç”±ç¤¾åŒºç»´æŠ¤çš„ç¡¬ä»¶æ’ä»¶ï¼Œç”¨äºåœ¨ Ascend "
"NPU ä¸Šè¿è¡Œ vLLMã€‚é€šè¿‡æ­¤ç‰ˆæœ¬ï¼Œç”¨æˆ·ç°åœ¨å¯ä»¥åœ¨ Ascend NPU ä¸Šäº«å—åˆ° vLLM çš„æœ€æ–°"
"åŠŸèƒ½å’Œæ”¹è¿›ã€‚"

#: ../../user_guide/release_notes.md:283
msgid ""
"Please follow the [official doc](https://vllm-ascend.readthedocs.io/en/"
"v0.7.1-dev) to start the journey. Note that this is a release candidate, "
"and there may be some bugs or issues. We appreciate your feedback and "
"suggestions [here](https://github.com/vllm-project/vllm-ascend/issues/19)"
msgstr ""
"è¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https://vllm-ascend.readthedocs.io/en/v0.7.1-dev)å¼€å§‹æ‚¨çš„ä½“"
"éªŒä¹‹æ—…ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›æ¼æ´æˆ–é—®é¢˜ã€‚æˆ‘ä»¬éå¸¸æ¬¢è¿æ‚¨"
"åœ¨[è¿™é‡Œ](https://github.com/vllm-project/vllm-ascend/issues/19)æäº¤åé¦ˆå’Œå»º"
"è®®ã€‚"

#: ../../user_guide/release_notes.md:287
msgid ""
"Initial supports for Ascend NPU on vLLM. [#3](https://github.com/vllm-"
"project/vllm-ascend/pull/3)"
msgstr ""
"åœ¨ vLLM ä¸Šåˆæ­¥æ”¯æŒ Ascend NPUã€‚[#3](https://github.com/vllm-project/vllm-"
"ascend/pull/3)"

#: ../../user_guide/release_notes.md:288
msgid ""
"DeepSeek is now supported. [#88](https://github.com/vllm-project/vllm-"
"ascend/pull/88) [#68](https://github.com/vllm-project/vllm-ascend/pull/68)"
msgstr ""
"ç°åœ¨å·²æ”¯æŒ DeepSeekã€‚ [#88](https://github.com/vllm-project/vllm-ascend/"
"pull/88) [#68](https://github.com/vllm-project/vllm-ascend/pull/68)"

#: ../../user_guide/release_notes.md:289
msgid ""
"Qwen, Llama series and other popular models are also supported, you can see "
"more details in [here](https://vllm-ascend.readthedocs.io/en/latest/"
"user_guide/supported_models.html)."
msgstr ""
"Qwenã€Llama ç³»åˆ—åŠå…¶ä»–æµè¡Œçš„æ¨¡å‹ä¹Ÿå—æ”¯æŒï¼Œæ›´å¤šè¯¦æƒ…å¯å‚è§[è¿™é‡Œ](https://vllm-"
"ascend.readthedocs.io/en/latest/user_guide/supported_models.html)ã€‚"

#: ../../user_guide/release_notes.md:293
msgid ""
"Added the Ascend quantization config option, the implementation will coming "
"soon. [#7](https://github.com/vllm-project/vllm-ascend/pull/7) [#73]"
"(https://github.com/vllm-project/vllm-ascend/pull/73)"
msgstr ""
"æ–°å¢äº† Ascend é‡åŒ–é…ç½®é€‰é¡¹ï¼Œå…·ä½“å®ç°å³å°†æ¨å‡ºã€‚[#7](https://github.com/vllm-"
"project/vllm-ascend/pull/7) [#73](https://github.com/vllm-project/vllm-"
"ascend/pull/73)"

#: ../../user_guide/release_notes.md:294
msgid ""
"Add silu_and_mul and rope ops and add mix ops into attention layer. [#18]"
"(https://github.com/vllm-project/vllm-ascend/pull/18)"
msgstr ""
"æ·»åŠ  silu_and_mul å’Œ rope æ“ä½œï¼Œå¹¶å°†æ··åˆæ“ä½œåŠ å…¥åˆ° attention å±‚ã€‚ [#18]"
"(https://github.com/vllm-project/vllm-ascend/pull/18)"

#: ../../user_guide/release_notes.md:298
msgid ""
"[CI] Enable Ascend CI to actively monitor and improve quality for vLLM on "
"Ascend. [#3](https://github.com/vllm-project/vllm-ascend/pull/3)"
msgstr ""
"[CI] å¯ç”¨ Ascend CIï¼Œä¸»åŠ¨ç›‘æµ‹å¹¶æå‡ vLLM åœ¨ Ascend ä¸Šçš„è´¨é‡ã€‚[#3](https://"
"github.com/vllm-project/vllm-ascend/pull/3)"

#: ../../user_guide/release_notes.md:299
msgid ""
"[Docker] Add vllm-ascend container image [#64](https://github.com/vllm-"
"project/vllm-ascend/pull/64)"
msgstr ""
"[Docker] æ·»åŠ  vllm-ascend å®¹å™¨é•œåƒ [#64](https://github.com/vllm-project/"
"vllm-ascend/pull/64)"

#: ../../user_guide/release_notes.md:300
msgid ""
"[Docs] Add a [live doc](https://vllm-ascend.readthedocs.org) [#55](https://"
"github.com/vllm-project/vllm-ascend/pull/55)"
msgstr ""
"[æ–‡æ¡£] æ·»åŠ äº†ä¸€ä¸ª [åœ¨çº¿æ–‡æ¡£](https://vllm-ascend.readthedocs.org) [#55]"
"(https://github.com/vllm-project/vllm-ascend/pull/55)"

#: ../../user_guide/release_notes.md:304
msgid ""
"This release relies on an unreleased torch_npu version. It has been "
"installed within official container image already. Please [install](https://"
"vllm-ascend.readthedocs.io/en/v0.7.1rc1/installation.html) it manually if "
"you are using non-container environment."
msgstr ""
"æ­¤ç‰ˆæœ¬ä¾èµ–äºå°šæœªå‘å¸ƒçš„ torch_npu ç‰ˆæœ¬ã€‚è¯¥ç‰ˆæœ¬å·²é›†æˆåœ¨å®˜æ–¹å®¹å™¨é•œåƒä¸­ã€‚å¦‚æœæ‚¨"
"ä½¿ç”¨çš„æ˜¯éå®¹å™¨ç¯å¢ƒï¼Œè¯·[æ‰‹åŠ¨å®‰è£…](https://vllm-ascend.readthedocs.io/en/"
"v0.7.1rc1/installation.html)ã€‚"

#: ../../user_guide/release_notes.md:305
msgid ""
"There are logs like `No platform detected, vLLM is running on "
"UnspecifiedPlatform` or `Failed to import from vllm._C with "
"ModuleNotFoundError(\"No module named 'vllm._C'\")` shown when running vllm-"
"ascend. It actually doesn't affect any functionality and performance. You "
"can just ignore it. And it has been fixed in this [PR](https://github.com/"
"vllm-project/vllm/pull/12432) which will be included in v0.7.3 soon."
msgstr ""
"åœ¨è¿è¡Œ vllm-ascend æ—¶ï¼Œä¼šæ˜¾ç¤ºç±»ä¼¼ `No platform detected, vLLM is running on "
"UnspecifiedPlatform` æˆ– `Failed to import from vllm._C with "
"ModuleNotFoundError(\"No module named 'vllm._C'\")` çš„æ—¥å¿—ã€‚è¿™å®é™…ä¸Šä¸ä¼šå½±å“"
"ä»»ä½•åŠŸèƒ½å’Œæ€§èƒ½ï¼Œä½ å¯ä»¥ç›´æ¥å¿½ç•¥å®ƒã€‚è¿™ä¸ªé—®é¢˜å·²åœ¨æ­¤ [PR](https://github.com/"
"vllm-project/vllm/pull/12432) ä¸­ä¿®å¤ï¼Œå¹¶å¾ˆå¿«ä¼šåœ¨ v0.7.3 ç‰ˆæœ¬ä¸­åŒ…å«ã€‚"

#: ../../user_guide/release_notes.md:306
msgid ""
"There are logs like `# CPU blocks: 35064, # CPU blocks: 2730` shown when "
"running vllm-ascend which should be `# NPU blocks:` . It actually doesn't "
"affect any functionality and performance. You can just ignore it. And it "
"has been fixed in this [PR](https://github.com/vllm-project/vllm/"
"pull/13378) which will be included in v0.7.3 soon."
msgstr ""
"åœ¨è¿è¡Œ vllm-ascend æ—¶ï¼Œä¼šæ˜¾ç¤ºç±»ä¼¼ `# CPU blocks: 35064, # CPU blocks: 2730` "
"çš„æ—¥å¿—ï¼Œå®é™…åº”è¯¥ä¸º `# NPU blocks:`ã€‚è¿™å®é™…ä¸Šä¸ä¼šå½±å“ä»»ä½•åŠŸèƒ½å’Œæ€§èƒ½ï¼Œä½ å¯ä»¥å¿½"
"ç•¥å®ƒã€‚è¯¥é—®é¢˜å·²åœ¨è¿™ä¸ª [PR](https://github.com/vllm-project/vllm/pull/13378) "
"ä¸­ä¿®å¤ï¼Œå¹¶å°†åœ¨ v0.7.3 ç‰ˆæœ¬ä¸­åŒ…å«ã€‚"
