test_name: "test DeepSeek-R1-W8A8 on A2"
model: "vllm-ascend/DeepSeek-R1-0528-W8A8"
num_nodes: 2
npu_per_node: 8
env_common:
  VLLM_ASCEND_ENABLE_MLAPO: 1
  VLLM_ASCEND_BALANCE_SCHEDULING: 1
  HCCL_INTRA_PCIE_ENABLE: 1
  HCCL_INTRA_ROCE_ENABLE: 0
  PYTORCH_NPU_ALLOC_CONF: expandable_segments:True
  VLLM_USE_MODELSCOPE: true
  HCCL_BUFFSIZE: 200
  SERVER_PORT: 8080
  OMP_PROC_BIND: false
  OMP_NUM_THREADS: 1


deployment:
  -
    server_cmd: >
      vllm serve vllm-ascend/DeepSeek-R1-0528-W8A8
      --host 0.0.0.0
      --port $SERVER_PORT
      --data-parallel-size 4
      --data-parallel-size-local 2
      --data-parallel-address $LOCAL_IP
      --data-parallel-rpc-port 13399
      --no-enable-prefix-caching
      --max-num-seqs 16
      --tensor-parallel-size 4
      --max-model-len 16384
      --max-num-batched-tokens 4096
      --enable-expert-parallel
      --async-scheduling
      --trust-remote-code
      --quantization ascend
      --gpu-memory-utilization 0.92
      --speculative-config '{"num_speculative_tokens": 3, "method":"mtp"}'
      --compilation-config '{"cudagraph_capture_sizes":[4,16,32,48,64], "cudagraph_mode": "FULL_DECODE_ONLY"}'

  -
    server_cmd: >
      vllm serve vllm-ascend/DeepSeek-R1-0528-W8A8
      --headless
      --data-parallel-size 4
      --data-parallel-rpc-port 13399
      --data-parallel-size-local 2
      --data-parallel-start-rank 2
      --data-parallel-address $MASTER_IP
      --no-enable-prefix-caching
      --max-num-seqs 16
      --tensor-parallel-size 4
      --max-model-len 16384
      --max-num-batched-tokens 4096
      --enable-expert-parallel
      --async-scheduling
      --trust-remote-code
      --quantization ascend
      --gpu-memory-utilization 0.92
      --speculative-config '{"num_speculative_tokens": 3, "method":"mtp"}'
      --compilation-config '{"cudagraph_capture_sizes":[4,16,32,48,64], "cudagraph_mode": "FULL_DECODE_ONLY"}'
benchmarks:
